[
["index.html", "Archaeological Science with R Chapter 1 Archaeological Science with R", " Archaeological Science with R Ben Marwick 2019-05-28 Chapter 1 Archaeological Science with R This book will teach you how to get started doing archaeological science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for archaeological science. To be published by XXX in XXX 201X. Here is a summary of the progress of the book: Number Title Wordcount last_edit 01 Introduction 7413 2019-05-27 22:40:44 02 Writing reproducible research 8179 2019-05-27 22:43:31 03 Collecting data for R 4696 2019-05-27 22:40:44 04 Getting data into R 7454 2019-05-27 23:51:06 05 Cleaning and tidying data with R 5879 2019-05-28 00:28:47 06 Visualising data with R 1162 2019-05-27 22:40:44 07 Analysing artefact data with R 86 2019-05-27 22:40:44 08 Analysing Excavation and stratigraphic data with R 23 2019-05-27 22:40:44 09 Analysing spatial data and making maps with R 57 2019-05-27 22:40:44 "],
["introduction.html", "Chapter 2 Introduction 2.1 Overview 2.2 Why do archaeologists need to learn to code? 2.3 Why R? 2.4 Who should read this book 2.5 Prerequisites 2.6 Key terms to understand (don’t skip this bit!) 2.7 Getting help", " Chapter 2 Introduction 2.1 Overview The goal of “Archaeological Science with R” is to give you a solid foundation into using R to do archaeological science. By archaeological science, I mean, systematic, objective, and empirical research into past human behaviours using data collected from material remains and traces. The goal is not to be exhaustive, but to instead focus on what I think are the critical skills for doing archaeological science with R. Some archaeologists already use R in an ad hoc way, for a quick plot here, or a linear model there. If you are one of these people, you will see some familiar things in this book. But you will also see a lot of new ideas, because this book will show you how R can be at the center of your entire research workflow, from when you start working with raw data until your final thesis, report, or manuscript is complete. 2.2 Why do archaeologists need to learn to code? This book aims to solve a specific problem. The problem is that the majority of archaeologists receive little or no training in scientific programming for data analysis and visualization, and yet they routinely analyse and visualise data. A command-line interface program such as R is ideal for this kind of work, and yet is unfamiliar and exotic to most archaeologists. A command-line interface refers to interacting with software by sending instructions to the program as lines of plain text. Instead, most archaeologists use Microsoft Excel, SPSS, or similar commercial point-and-click software. There are four problems with this. It usually results in a lot of time-consuming repetition, like copying-and-pasting between sheets in Excel, and between Excel and Microsoft Word. In recent years, biologists have seen great increases in volumes of genomic data, due to improvements in sequencing technology. This has led them to search for more efficient ways to analyse their data, and automate their analyses. They found that spreadsheet programs did not provide enough flexibility to conduct their analyses. As a result, many have turned to R, Python, and similar programming languages to overcome the limitations and inefficiencies of Excel. The key detail here is a shift from the point-and-click interface of a spreadsheet program, to the command-line interface of a programming language where we supply instructions to the computer in plain text. It limits the development of new methods because the you are limited to what is available in the commercial software. With most commercial software packages, you are limited to the suite of functions they choose to make available to you. With an open source programming language, you and anyone else are free to implement new methods. R has extensive functions for data analysis. This includes features likes missing values and subsetting. But more importantly, R has a large set of packages (currently &gt;10,000) for quantitative analysis, visualisation, and importing and manipulating data. For most archaeologists, whatever analysis or plot you are attempting, chances are that someone has already tried it, or something very similar, and made the code available in an R package. R is also the lingua franca for researchers in statistics, who will often publish an R package to accompany their scholarly articles. This often means immediate access to the very latest statistical techniques and implementations. Commercial software (and even some free software) also makes it difficult to demonstrate and ensure reproducibility due to the ephemeral nature of point-and-click interfaces. Point-and-click interfaces are familiar and easy to use for most people because they are very common in software programs. But it is very challenging to efficiently record a sequence of complex clicks so that another person (or you in the future) can unambiguously repeat the procedure. This means it is difficult to make your analyses reproducible if all your work is done with a mouse. It is not practical to completely abandon using a mouse when using a computer, but by using a command-line program such as R, we can record the most important steps of the analysis workflow in plain text code. Then we have an detailed record of the steps in our analysis that we can easily share with others, and re-use ourselves, months or years into the future Commercial software limits transparency in research because the algorithms behind the functions are not available for convenient inspection and modification. Staticians have long noted that Excel has flawed statistical procedures (McCullough and Heiser 2008, @yalta2008accuracy), and introductory texts on statistics warn students not to use Excel when the results matter (eg. Keller 2000). With an open source program such as R, you can easily inspect and alter the algorithms behind every operation. R in particular has the added advantage of being one of the most accurate software programs for statistical analysis (Almiron, Almeida, and Miranda 2009, @keeling2007comparative). Using a command-line program such as R goes a long way towards solving these four problems. This book is complimentary to the major textbooks and handbooks of quantitative archaeology (Baxter 2003; 1994, Van Pool and Leonard 2011, Drennan 2009, Shennan 1997). These are excellent for relevant statistical theory, discussion, and examples, and this book does not replace them on your shelf. However, those books have three limitations that we will address in the following chapters. First is the absence of ‘plug-and-play’ examples, leading the reader to invest substantial effort to implement a method described using formulae, creating many opportunities for error. Van Pool and Leonard 2011 instruct their reader to do their statistical analyses by hand, an impractical and outmoded recommendation when computers are readily available. The amount of effort required by these texts is prohibitive to rapid exploration and experimentation of data using new methods. This book includes reproducible examples using real data sets so you can easily step-through the analysis to explore the effects of different parameters, and easily interchange your own data. The existing books give little coverage to intuitive and methodologically more robust methods such as resampling and Bayesian analyses, favoring traditional parametric methods. This reflects a time when computational power was scarce, and computations often done by hand or with a calculator (cf Fletcher and Lock 1991). Modern computers can now easily handle resampling and Bayesian methods, and R is unique in having a mature set of methods for computing common tests in these frameworks. These methods are also increasingly common in the published research literature. This book introduces these alternative approaches to give you more options in your analytical toolkit. The currently available books are silent on the practical mechanics of many common data analysis and visualization tasks in archaeology that are not traditionally considered statistics. This includes displaying and analyzing stratigraphic information from excavation and spatial data from surveys. Learning a tool-chain for these tasks is traditionally done on your own, and often at substantial expense with proprietary software. This book demonstrates how to write simple programs that are especially useful for archaeologists. By doing this, and providing code for other common tasks, this book addresses the need for instruction in a comprehensive open source tool-chain for these common tasks in archaeological science. It does this by presenting a reproducible research environment to show how R and its contributed packages can be used for start-to-finish research into survey, excavation, and laboratory data. My hope is that a practical and accessible introduction to reproducible research, such as this book aims to provide, will improve openness and transparency in archaeology generally. It will contribute towards creating a community of researchers where it it normal and routine to publish code and data (after appropriate precautions are taken to protect sensitive information) with reports and publications, so that we can engage more deeply with, and learn more efficiently from, each other. This is not a book of detailed discussions of statistical theory, and you will be referred to other texts on technical details of algorithms, etc. This is a book introducing R for practical and efficient implementations of common tasks in archaeological science, and serves as a springboard to more advanced R programming. The specific topics covered in this book are: Implementing reproducible research with literate programming as a practice that is good for science generally, and good for your individual productivity Working with common archaeological data structures, ingesting them into R and manipulating them from messy formats to tidy formats ready for analysis Working with data from stone artefacts, faunal remains, pottery, glass, and metal artefacts Compositional analysis using cluster and principle components analyses of multivariate data Working with relative and absolute chronologies by doing seriation, and calibrating, analysing and visualising radiocarbon dates Visualising and analysing stratigraphic data from archaeological excavation Visualising spatial data by making maps, doing spatial analysis and site classification Simplifying collaborative research with version control 2.3 Why R? If you are new to R, you might wonder what the appeal is, especially when there are so many programs and languages in common use. Some of the characteristics that drew me to learn R include: It’s free, open source, and available on every major type of computer. As a result, if you do your analysis in R, there is a very good chance that anyone with a computer can easily reproduce it. You do not have to purchase a license or subscription to use R. It is specialised for use with statistical analysis and data visualisation. R was originaly develped by statisticians, and continues to be widely used by professional and academic statisticians. This means that there are strong links between the statistical literature and R code. Many of its algorithms have been vetted by publication and thoroughly tested through extensive use. It is easy to go from statistical theory to practice using R. An active, supportive and generally progressive community. It is easy to get help from experts on the internet. You can also connect with other R learners via social media, and through many local user groups. I have found Stack Overflow (more on this below) and twitter to be particularly useful sources of information. Package authors are often pleased to see their packages used by others, and willing to answer questions about your use of their package. Flexible tools for communicating your results. R packages make it easy to produce html, pdf, or Microsoft Word documents that include text, tables and figures, all generated from R code. I find this to be a substantial time-saver, especially when weeks or months pass between working on a project, and I cannot remember all the details of the last work I did. When I look over my R code I can quickly recall my analysis plan and resume from where I stopped. 2.4 Who should read this book This book is written for archaeologists who are keen to expand their analytical horizons and gain access to new methods and more efficient ways of organising their research process. No prior knowledge about R, computer science or programming is expected, but a curiosity about statistics would be an advantage, as well as a readiness to read beyond this book to make decisions about the suitability of statistical methods for your specific research questions. This book is intended for readers looking for practical applications of R for archaeological science. For general-purpose, gentle introductions to R, take a look at De Vries and Meys (2015) and Braun and Murdoch (2016). If you have some programming experience, and are looking for a more comprehemsive introduction to the R language, I recommend Matloff (2011) and Wickham (2014). Most of the methods presented here are foused on rectangular data, that is, data that are typically entered into spreadsheet and database files. More specifically, the book is aimed at archaeolgists using data that is organised into rows (i.e. variables) and columns (i.e. specimens, samples or observations). There are lots of archaeological datasets that are organised differently, such as images and text. Although R is also useful for those, we do not discuss them in detail here. Rectuangular data are very common in archaeology, and many other fields, so that is why we concentrate on those here. This book is useful for archaeologists working with small data (i.e. &lt; 1 million rows, &lt; 2 Gb per file) that can be stored on your computer (rather than a remote data appliance). In my experience, the majority of archaeologists work with data at this scale, so the methods presented here will be useful for the majority of archaeological applications. If your data are bigger than this, you are probably doing something very specialised and will have unique challenges to make your research reproducible. However, you can still use R and the methods decribed here for working on subsets of your large dataset. 2.5 Prerequisites To run the code in this book, you will need to have R installed on your computer, as well as RStudio, an application that makes it substantially easier to use R. Although I have been interested in R for many years, it was only after RStudio was first released that R replaced Excel and SPSS for me. Both R and RStudio are free to download and easy to install. You can install it on most recent versions of Windows, OSX (Mac) or Linux. When used together, R and RStudio will help you to have a smooth and efficient experience when programming. RStudio provides many features that greatly simplify programming with R (my favourite is tab-completion). I do not recommend using R without RStudio, especially for the beginner. You should download and install both R and RStudio before attempting to follow any of the examples in this book. If you have installed these programs on your computer in the past, you should update them now (new versions of R are released every six months). 2.5.1 R To install R, visit cran.r-project.org. CRAN is the ‘Comprehensive R Archive Network’ which hosts the official releases of the R software and documentation. Then click the link that matches your operating system. What you do next will depend on your operating system. You will need to install a development environment, this is also specific to your operating system. The development environment provides some additional programs that give you access to more advanced features of the R language. The examples in this book have been tested to work in R version 3.6.0 (2019-04-26), and may not work in earlier versions. Here’s a summary of the basic steps to download and install R for common operating systems: Mac users should click the most current release. This will be the .pkg file at the top of the page. Once the file is downloaded, double click it to open an R installer. Follow the directions in the installer to install R. You should also install Xcode from the Mac App Store to install the development environment. Windows users should click “base” and then download the most current version of R, which will be linked at the top of the page. You should also install Rtools to install the development environment. Linux users will need to select their distribution and then follow the distribution specific instructions to install R. cran.r-project.org includes these instructions along side of the files to download. If you are not sure about these download and install steps then you should research the general methods of installing software for your operating system (e.g. how-to videos on youtube), which are beyond the scope of this book. After installing R on your computer, you may be tempted to open it and start exploring. I don’t recommend this because the R user interface is not intuitive for novices. Instead, continue reading below about downloading and installing RStudio. Then start your explorations in RStudio, which has a more user-friendly interface. 2.5.2 RStudio Once you have R installed, it is time to download RStudio. To download RStudio, visit www.rstudio.com/download. Choose the installer that matches your operating system. Then click the link to download the application. Once you have the application downloaded, installation is the same as for most programs on your computer. Once RStudio is installed, open it as you would open any other application. It should look something like this: knitr::include_graphics(&quot;images/RStudio_first_view.jpg&quot;) Figure 2.1: A first view of RStudio. When you start RStudio for the first time, you’ll see your screen divided into three or four window panes. If you don’t see four window panes similar to the image above, go to File -&gt; New File -&gt; R Script to open a new R script file and show all four panes. One setting that is very important to change before doing any work in RStudio is how .RData files are handled. An .RData file can store the data and functions that you create in each R work session. By default, RStudio automatically saves this file when you exit RStudio, and reloads it when you start RStudio. I have only ever found this to be a source of problems and confusion. This is because it can result in temporary objects that were created during brief experiments and explorations lingering across multiple sessions as they are automatically reloaded, contaminating the workspace. So I recommend you change this default by going to the ‘Tools’ drop-down menu, select ‘Global Options…’, look in the ‘General’ section and making two changes. The first change is to uncheck ‘Restore .RData into workspace at startup’, and the second change is at the item ‘Save workspace to .RData on exit’, you should select ‘Never’. Changing these two settings will save a lot trouble as you learn how to use R’s environment, and help you to take control of the data and functionsthat you make with R. In most cases it is simplest and most efficient to treat the code that you write as the most important product while you work on analysis, and write code to export (or save) only the most important results (for example some tabular output as CSV files, and plots as PNG files), rather than automatically saving the entire workspace and all ther intermediate objects that you create along the way. knitr::include_graphics(&quot;images/rstudio_never_restore.png&quot;) Figure 2.2: An important setting to change before using RStudio. Here is an explanation of what you see in Figure ??. For more details about the RStudio interface, see the RStudio website https://www.rstudio.com/products/rstudio, Verzani (2011), Gandrud (2015), and Racine (2012): The top left pane is the text editor, known as the ‘source’. This is where you write and edit code and text. The text editor in RStudio is analogus to a word processor such as Microsoft Word, but has many additional convienent features for writing and editing code. Writing code means writing in plain text, there is no bold, underline, italics or other formatting. In Chapter XX we will discuss how we can produce nicely formatted text in Microsoft Word documents using this text editor and the Rmarkdown document preparation system. We can modify the appearance of the text editor in RStudio by going to the ‘Tools’ drop-down menu, and selecting ‘Global Options…’, then browsing the options in the ‘Code’ and ‘Appearance’ sections on the left side list. I find it useful to show whitespace characters (in the Code section, Display tab), and check all the completion and diagnostic options (Code section Completeion tab and Diagnostics tabs). The code in the text editor is the most important product of your work in RStudio, so it’s a good idea to save it in a sensible place, with a meaningful name (usually R code files end in .R, with a capital R after the period), and to save it frequently while you’re working on it to minimise the risk of accidental loss. The top right pane can have several tabs, depending on what you are doing. Most of the time you will see an ‘Environment’ tab and a ‘History’ tab. The Environment tab lists the objects that are currently available in your R session. The ‘environment’ is the place where R stores the results of calculations and functions. For example, if you have executed code that reads an Excel spreadsheet into R and stores it as an R object, then you will see this object listed in the Environment tab. This tab is useful because you can get basic information at a glance about the size of objects that you make (e.g. how many rows and columns). This is convienent for quickly checking the output of your computations so you can see if your code is working as expected. The environment is an important aspect of R because it gives us a coherent and flexible system for creating, combining and manipulating different types of data during our analysis. Environments are an advanced programming concept, and all we need to do here is note that they are helpful, and that there is nothing comparable to them in spreadsheet programs. The History tab shows the code you have previously executed in your R session. I rarely use this tab because I write code into the text editor and save the code to my computer, so I always have a copy of the code in the text editor. I strongly recommend saving your code in a .R file for even the briefest analyses and experiments. But if you are working in an expedient fashion without saving, you can use the History tab to browse your previous commands to reuse and edit. You can also save the history of your R commands to a text file, but I don’t recommend this as it is not an efficient method for keeping track of your work. A better strategy is to write code in the text editor, interspersed with comments (lines of text prefaced with the # character) that explains what the code is doing, and save that document to your computer. We’ll discuss this strategy more in chapter XX. The lower right pane is another multi-tab pane where plots appear, the most important are the Plots tab and the Files tab. The Plots tab allows you to conveniently browse the plots that you produce. You can navigate back to previous plots created in your current session, and interact with plots by zooming and, in some cases selecting, rotating, etc. The Plots tab has an ‘Export’ button to easily save your plots as image files on your computer (in a variety of formats), or copy to your clipboard for a quick copy-paste transfer. These are useful features for exploring pictures of your data and iterating towards a publication-quality plot. The lower left pane is the console where code is executed. There are no buttons specific to the console, so direct interaction with this pane is limited. The most important part of this pane is the prompt, which looks like this &gt;, and when this pane is active, there is a blinking cursor at the prompt. The space where the cursor is in is called the command line, where you type, paste, or send code. When you type at the prompt and press Enter, you are submitting code to be interpreted by R, and R will usually return the result in the console. Here is one of the simplest possible examples of using the console: &gt; # this is a line of text, indicated by the initial # character, it is ignored by R &gt; 1 + 1 # I typed 1 + 1 in the console and then pressed the &#39;enter&#39; key on my keyboard [1] 2 In this example, I placed my cursor at the prompt, typed 1 + 1 in my RStudio conosole with my keyboard, and then pressed Enter. The R interpretor returned the result, 2, directly below my input. The [1] with the square brackets indicates that this is the first item returned by the interpretor. At first glance, this [1] might seem unessecary, but this indexing is helpful when working with more complex calculations that produce more extensive output. Most of the time you wont type directly at the prompt because it is not efficient for more complex code. Instead, you’ll type code in the text editor, and send that code from the text editor to the prompt by pressing the Control + Enter keys simutanteously (on OSX: Command + Enter). There are two useful features of the console that will save you time when working with R. First is the up arrow key on your keyboard, which allows you to re-run previous commands. If you place your cursor at the prompt, and press the up arrow key, you can browse the previously executed lines of code. Once you find the code you want to re-run, you can edit it in the console, if you wish, or simply press Enter to send it to the interpreter. The second handy feature of the console pane in RStudio is the small arrow icon at the right side of the title bar of the console. If you look at the top of the console pane, you’ll see the word “Console”, then you’ll see a path to a folder on your computer, then at the end of that, you’ll see a little curved arrow icon pointing to the right. The path on your computer in this console title bar is your ‘Working Directory’. If you click on the small arrow, it will switch the Files pane to show the contents of your Working Directory. The Working Directory is an important concept to understand because it is unlike anything you might have seen in Excel or other point-and-click software. The Working Directory is the directory (or folder) on your computer where R is currently working. It is the location on your computer where R will look when you read files into R, or save output from R to your computer. By default, your R working directory was probably set to an inconvienent directory when you installed R (this is always the case for me), but this is easily changed. The simplest method is to go to the RStudio toolbar menu, click on ‘Session’, then ‘Set Working Directory’ then ‘To Source File Location’. You may see in other people using R code to set the working directory (setwd()), but I don’t recommend this. If you use code to set the working directory, your code will only work on your computer, in its current configuration of files and folders. This means that your code is not portable and not robust to change. If you reorganise your files, or give your code to a collaborator, the actual working directory whre your files are will no longer match what is written in the code. This can lead to errors and frustration. So while the concept of the Working Directory is important to know about for working with R, you should not include it in your R code. You may need your code to refer to other directories relative to your main project directory (for example, a directory called ‘data’ that contains spreadsheets, etc.), but that is a different task to setting the Working Directory, and we’ll look into that in Chapter XX. Although I have described the default appearance, the location and combination of panes and tabs in RStudio are customisable via the toolbar menu (Tools -&gt; Global Options -&gt; Pane Layout). For example, I prefer to have to console on the upper right, and I often minimize some of the panes so I only see the text editor and the console. This makes it easier for me to keep track of what I’m doing and I don’t get distracted by unnecesary details on my screen. I also like to the use the PragmataPro font with RStudio because it I find it easier to read code in this font compared to many others. 2.6 Key terms to understand (don’t skip this bit!) Now we’ve established the basic motivation for learning R to do archaeology, and introduced the layout of the software, we should introduce some key terms and conventions that appear frequently in this book. Becoming familiar with these terms will help you with the basic tasks of finding your way around when starting to use R. We have already discussed the prompt and command line in the previous section, and we’ve used the word code, but without really defining it. Code refers to anything written in a programming language, from a single word to a function with thousands of lines. Code often contains comments, which you can recognise in the R language by a # symbol at the start of the line. Code comments are important because they help you and other people understand the purpose of the code. The # symbol tells the R interpretor to ignore to everything to the right of that symbol, and skip to the next line of code. There are no strict rules on how many comments your should include with your code, but a good starting point would be one line of comment for each 10 lines or major section of code. More generally, your comments should try to tell you what you need to know a few weeks or months from now when you return to work on the code after a break, and can’t recall the minutae of your previous code. A script is a file that contains code, and R script file names ends with .R. You can open these files in any program that edits text, for example Notepad on Windows (or Notepad++) or TextEdit on OSX (or Sublime Text). A function refers to a group of code that carries out instructions to do useful work. For example, the function mean() computes the mean, or average, of a set of numbers. In this book you will always be able to recognise a function because they will always be followed by a pair of parentheses, often with code in between the parentheses. A function is a efficient way to organise code, because a single function can cause hundreds of lines of code to run to produce a result. Rather than running those hundreds of lines, one-by-one, over and over, we can simply type the function name that invokes them, and save ourself a lot of typing. Minimising typing is good because it saves time, and lowers the chances that we might introduce errors into our analysis by making typing mistakes. Usually we want to save the output of a function so we can use it in other calculations, for plotting, or to share with our colleagues. For example, we might want to compute the mean of the lengths of an assemblage of artefacts. To keep the results of function, we assign the output to an object, this process of assignment saves the output in our environment so it is available for reuse later in our session. Assignment does not save data to a file that can be used elswhere, such as emailed as an attachment, that is a separate process called ‘exporting’ data from R which we’ll cover later. This example demonstrates the use of two simple functions to compute a mean value: x &lt;- c(4, 7, 12) # length measurements of three artefacts y &lt;- mean(x) # compute the mean length The c() is a function to combine (‘c’ is for combine) our artefact measuremens into a type of set called a vector. A vector is a single sequence of data elements of the same type. In our example above all elements are numbers (as in our example above). There are two other commonly used vectors in R: a character vector where all elements could be letters or words, and a logical vector where all elements are logical constants, either TRUE or FALSE. The vector is a fundamental data type in R that we will use very often. In the above example we have stored the result of the c() function in an object that we call x (we can call it almost anything that we like, and we do not need to create x in advance). The second line of code in the above example computes the mean of 4, 7, and 12 (ie. (4 + 7 + 12)/3), and then assigns, or stores, the result in a object called y. We can then use both x and y later in our workflow for other tasks. This is useful because it means we don’t have to recompute the mean repeatedly. An important new concept in this example is the assignment operator, &lt;-, which we can translate as ‘put the results from the right side of this symbol into the thing on the left side’. Note that in the above example I have not included the prompt character (&gt;). Omitting this character makes it easier for you to copy and paste from this book into your R console so you can run the code yourself, and explore what happens when you make minor changes. There’s a lot more vocabularly to come, but these are the key terms that you need to know to make sense of the rest of this book and to get started exploring R by yourself. 2.6.1 Packages Functions, such as c() and mean() in the above example, are at the heart of working with R, and are part of the reason for R’s great versatility. They save a lot of time by minimizing typing and copy-and-pasting, so it’s worth to invest some effort into learning how to use functions, and how to write your own functions (more on that in Chapter XX). Functions are typically organised into packages, which you can download to extend the usefulness of R. We will use packages extensively in this book, and briefly discuss how and why you might write your own packages in Chapter XX. The most common way to install R packages is to run the function, install.packages() at the R prompt. For example, if you run install.packages(&quot;binford&quot;), you will automatically download the package binford from the repository at cran.r-project.org (you need to be connected to the internet) and install them in your system library. The binford package contains datasents from his 2001 book ‘Constructing Frames of Reference: An Analytical Method for Archaeological Theory Building Using Ethnographic and Environmental Data Sets’. Try this yourself by opening RStudio and running these commands to install some useful packages that we will use in later chapters. You only need to use install.packages() once (per computer) to download the package files. You don’t need to do this each time you open RStudio, but to ensure you have the lastest versions of the package you might want to run update.packages() every few months. Notice how we use the c() function here to create a character vector of package names for the install.packages() function to work on: # install a single package install.packages(&quot;devtools&quot;) # install multiple packages at one time install.packages(c(&quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;knitr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;scales&quot;, &quot;stringr&quot;, &quot;tidyr&quot;)) Most of this book uses packages from a group called the ‘tidyverse’, which can be installed altogher with install.packages(tidyverse). However, I prefer to mention the individual packages by name so you have a clearer understanding of where to look to get help on specific functions, and to help you discover related functions. Most of the packages we will use in this book are stored on cran.r-project.org, an repository maintained by a small group of expert R programmers. Other online repositories for R packages include Bioconductor, focusing on genetics software, also maintained by expert R programmers, and www.github.com, a free code-sharing website that where anyone can share their packages (without maintainers like CRAN and Bioconductor). You can install packages stored on GitHub with the install_github() function in the devtools package (which we installed using the commands above). To use the install_github() function, pass it a character string with the form &quot;&lt;github username&gt;/&lt;github repository name&gt;&quot;. For example, we can install the package ggbiplot to plot the output of a Principle Components Analysis like this: devtools::install_github(&quot;vqv/ggbiplot&quot;) # The two colons are a short-cut to avoid typing library(devtools) When R installs a package, it downloads the package to your system library. You must use the library() function to make the contents of the package available to your current R session. Each time you open RStudio you will need to run library() to use functions in packages you have previously installed. It’s good practice to have the library() functions among the first few lines of your code for a project, so other users can quickly see what packages they will need to have to prepare to run your code. For example, to use the functions in the dplyr package, you would need to first run library(&quot;dplyr&quot;) You will need to rerun this library() command each time you open a new R session that uses functions from the dplyr package. One of the strengths of R, the vibrant community of researcher-developers, is also one of its weaknesses. This is because it means that some packages are updated frequently, and others are not. Sometimes these updates can change how your code works, or stop it from working altogether. We’ll discuss some detailed solutions to this in Chapter XX. For now we’ll just note that it’s good practice to include the output of sessionInfo() in the results of your analysi because this tells you the specific version numbers for all the packakges used in your your analysis. This means that if there are breaking changes in some of the packages you’re using over the life of your project, you have a record of the last version of the packages that worked for you. You can install older versions of packages from CRAN using devtools::install_version(). 2.7 Getting help Developing fluency in a language like German or Chinese takes time, practice, and has ups and downs. Learning a programming language like R is a similar process, and you should anticipate ups and downs as R becomes an increasingly central part of your workflow. R comes with extensive built-in documentation, which is highly structured and very often contains example code that you can can run to explore how a function works. I find these runnable examples to be most useful part of the built-in documentation. You can access the built-in doucmentation with one of the following methods, which are run in the R console: ?mean # opens the help page for the mean function ?&quot;+&quot; # opens the help page for addition ?&quot;if&quot; # opens the help page for if ??plotting # searches for topics containing words like &quot;plotting&quot; ??&quot;regression model&quot; # searches for topics containing phrases like this These methods of getting help do not require an internet connection, and so are still useful when you are offline. If you are online, here are a number of additional options for seeking help when you get stuck, or have a question. The simplest case is when you execute a line of code, only to get a cryptic error message in the console. A good first reponse is to copy the text of the error message to your clipboard, and paste it into a Google search box in your web browser. In most cases, the top search results will be from Stack Overflow or one of the R mailing lists. Sometimes this will get you a quick solution. However, sometimes error messages are too generic, or too specific, and you can browse through many pages of search results without getting any useful ideas to solve your problem. When that happens, there are a few places online where you can give some more detail about your problem, and often get helpful responses. Stack Overflow is a free question-and-answer website that is focused on computer programming. Participants receive points for asking clear questions, and for giving useful answers. This adds a competitive element to participating in the Q&amp;A, and when you ask a well-formed question, several answers can appear very quickly from participants eager to earn points by being the best answer to your question (you award these points, as the question-asker). The R mailing lists are more traditional email-based fora, with archives at several places online. Highly skilled R programmers and R-using scientists from a variety of disciplines are active on both the R mailing lists and Stack Overflow. I prefer Stack Overflow because responses to my questions typically come quicker, and I find it easier to see when someone else’s question has been answered, compared to browsing the email list archives. I find that in the course of writing my question to submit to Stack Overflow, the process of simplifying my code into a small, self-contained example (a vital ingredient of a good question) leads me to discover the cause of my problem. Very often it’s as simple as a misplaced comma or bracket. Then I can answer my question before posting it and seeking help from others. Similarly, I often find that many of my questions have already been asked and answered on Stack Overflow. One of the big challenges in getting the most out of Stack Overflow and email list archives is to recognise how similar an existing question is to your current problem, and thus how useful the answers are to your specific issue. If you are sure that your problem is new and unique, and you want to submit a question, here is some general advice to help you get a useful answer quickly, and have a pleasant experience at the same time, from the Stack Overflow and R mailing lists communities: Spend some time browsing previous posts to understand the community norms. Online communities have specific cultural values that are often strongly held. While some of these are spelled out in posting guides, many are also unwritten, and can only be learnt through mindful reading of previous correspondence. If you are familiar with the norms, you are more likely to have an efficient and satisfying interaction with the community. Make sure your computer has the latest version of R and of the package (or packages) you are having problems with. It may be that your problem is the result of a recently fixed bug. Spend some time creating a simple reproducible example that captures the essence of your problem. This mean a simplified version of your problem that another person can copy and paste from their browser into their R console to reproduce the error that you see. This can be quite an art-form, but the basic ingredients for a good reproducible example are: The packages you are using, e.g. library(dplyr) A small data set. R comes with many example data sets built-in, if you run data() you can see a list. These are very convenient to use for reproducible examples because you can count on everyone having them. If you want to include your own data, you can use dput() to generate R code that another person can copy and paste to recreate your data set. Code. Your code should be easy to read, with spaces between operators (+, -, *, etc.) and after commas, and conformant to a style guide, such as Hadley Wickham’s (2014) style guide in Advanced R. Your code should includes lines of commentary, which begin with #, to help others understand your problem. There should be just enough code to demonstrate your problem, don’t burden your audience with haaving to make sense of lots of your code that is not central to your question. A description of your R environment. The usual way to communicate your R environment (ie. your operating system type and version), is to include the output from sessionInfo() or devtools::session_info() (same output, but in a tidyer format) into your question. The reprex package by Jenny Bryan provides some convenient functions that greatly simplify this process of preparing reproducible examples for seeking help online. Don’t worry if some of these terms are unfamiliar at the moment. The main thing to know at this point is that help is available, and that the quality of help you receive is often directly proportional to the effort you spend seeking it. 2.7.1 Draft TOC Further reading: http://ellisp.github.io/blog/2017/01/14/books http://andrewgelman.com/2011/01/03/5_books/ Notes on common mistakes: http://arrgh.tim-smith.us/ https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md http://www.burns-stat.com/pages/Tutor/R_inferno.pdf Hypothesis testing: http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html https://ismayc.github.io/moderndiver-book (also 5NG, 5Mv) References "],
["writing-reproducible-research.html", "Chapter 3 Writing reproducible research 3.1 Overview 3.2 What does it mean to do reproducible research? 3.3 Literate programming 3.4 Summary", " Chapter 3 Writing reproducible research 3.1 Overview Reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original researcher (Goodman, Fanelli, and Ioannidis 2016). This is a cornerstone of the sciences because if we cannot reproduce previous studies, we cannot rely on their results, and we cannot build on them to generate new knowledge. But what does R have to do with such a weighty topic? The answer is that because we interact with R by writing commands in a script file, which we can save, share and archive, we are doing our analysis in a way that can be reproduced. This can be contrasted with analysing data in Excel, SPSS, PAST and other mouse-driven software where many of the intermediate steps are typically mouse-clicks that leave no trace. Another person inspecting your Excel file may struggle to identify the decisions you made in your data anlysis because of the many ephemeral and unrecorded steps that led to the final result. Why is reproducibility gaining attention? Four factors have lead to growing concerns about the reproducibility of scientific results in modern science. The first factor is reruns of landmark biomedical experiments that have failed to produce the same results as the initial publication of the experiment (Prinz, Schlange, and Asadullah 2011; Begley and Ellis 2012). The second factor is the discovery of high-profile studies where the published results were flawed due to simple errors data analysis(Baggerly and Coombes 2009; Herndon, Ash, and Pollin 2013). Third, there has been a sharp rise in the number of scholarly publications that have been retracted due to discoveries of misconduct such as fraudulent data and plagiarism (Van Noorden 2011; Steen 2013). Finally, there has been a growth of data-intensive research that use massive computer simulations or collect enourmous amounts of data from vast numbers of sensors (ie. astronomy and oceanography). These events have stimulated extensive discussion in many disciplines about how to improve the reproducibility of scientific research. Some of this discussion has resulted in calls from diverse dsicplines (Nosek et al. 2015) for publications to be accompanied by both the raw data that generated the figures in the paper (Reichman, Jones, and Schildhauer 2011), and the programming code that shows the steps taken in the data analysis (Barnes 2010; Ince, Hatton, and Graham-Cumming 2012; Morin et al. 2012). Related to these calls are a manifesto-like journal articles by groups of researchers that call for substantial changes in the research workflow to improve reproducibilty and transparency (Wilson et al. 2014; Hampton et al. 2015; Sandve 2013). I have explored these principles in more detail in Marwick (2016). In this chapter I show how archaeologists can respond to these calls for increased reproducibilty and transparency. I describe two key principles: literate programming for enabling reproducibilty, and version control for enhancing transparency. Literate programming refers to the use of a computing environment for authoring documents that contain a mix of natural (eg. English) and computer (eg. R) languages (Schulte et al. 2012). Conveniently, R and RStudio are well-equipped to support literate programming, so this practice is relatively easy to learn. The second section of this chapter introduces version control using Git as a principle and tool for enhancing transparency and collaboration. Version control of literate programming files allows you to keep track of all the changes that you and your co-authors make to your files. Using the Git software for version control, you and your collaborators can safely create multiple different versions of your files to experiment with, and either abandon the results, merge them with the main versions, or revert to an earlier version. Athough, neither of these are strictly part of R, R integrates very well with other tools that enable these principles. This makes R unique as an environment for working reproducibly with minimal effort. 3.2 What does it mean to do reproducible research? Before showing how we can make our research more reproucible, we should explore exactly what we mean by ‘reproducible’ because it is not widely used in archaeology. There is variation surrounding the use of the term ‘reproducibility’ in biology and computer science, especially in comparison to closely related terms such as replicability and reliability (Baker 2016). Here we follow Stodden’s [(2014); Stodden2016] defintion and taxnonomy of reproducibility, dividing the concept into three parts: empirical, computational, and statistical. The first part is empirical reproducibility, also called ‘replicability’. For an archaeologist, this means evaluating a previously obtained result by re-excavating a site that another researcher had previously excavated, studying a museum collection that had previously been analysed by another person, or repeating an experiement that has been previously published. Empirical reproducibility is about returning to the source of the data and physically collecting the data again. Archaeologists do this routinely, for example, we return to famous sites such as Star Carr, Catal Huyuk, and Niah Cave to excavate them again. Usually this happens decades after the original excavation that make the site prominent. This is not exact empirical reproducibility, because we do not use the exact procedures of the previous workers, but instead use modern field methods to extract more data from the site, and improve upon and extend the earlier work. But the general idea of testing the previous claims about the site is usually the key motivation for returning to it. The second and third parts of ‘reproducibility’ are computational and statistical reproducibility. Computational reproducibility refers to redoing the calculations of another researcher’s quantitative results using their original datasets and methods (i.e. code). This type of reproducibility does not involve independent data collection, but instead uses the methods and data collected by the original investigator. Enhancing computational reproducibility includes preseving the steps of how raw data are cleaned, tidied, and combined or separated in preparation for anlaysis and visualisation. Statistical reproducibility is enhanced when detailed information is provided about the choice of statistical tests, model parameters, threshold values, and other details that determine the outcomes of statistical analyses. Computational and statistical reproducibilty are less familiar to most archaeologists, because our disciplinary culture tends to value secrecy and privacy, rather than sharing and openness of the mundane details of our analyses. Many archaeologists view the production of a research publication like the writing of a novel. The creative processes and drafting of the novel is typically a personal, private process that the author keeps to themself. Similarly, the readers of the novel generally have no expectation to see these drafts or have any exposure to the creative process. The author and the reader have a shared understanding that the published novel is the final product, and no other materials from the creative process (e.g. earlier drafts, notes, outlines) are needed to make sense of it. This is because the novel is primarily an aesthetic product, its value coming from how it arouses the emotions of the reader. Early drafts of the novel are typically irrelevant to the aesthetic value of the final published novel. Of course there are exceptions, for example we often enjoy to hear the author of a novel speak about their inspirations and motives in interviews, and literary scholars find value in the notes and paraphenalia of writers. For the most part, however, these details are not part of the unspoken contract between the fiction writer and their readers Archaeological research, like other sciences, is fundamentally different from novel-writing, although traditional scholarly norms of not sharing anything except the final published article tend to reinforce the novel-writing model. The value of an archaeological publication is primarily not aesthetic, but in the truth-values of the claims made about human behvaiour in the past. In order to make a robust assessment of these claims, the reader may need to know details of the analytical process that are not presented in the final publication. There are many reasons why relavant details might not be inlcuded in a publcation. Primarily, we cannot anticipate and address every potential question in the limited space of a typical journal article. These kinds of private correspondences between readers and authors are common, and imply an interesting defintion of what data are in a research context. The implication is that that we as authors don’t have the final say on what data and method details are necessary to make our case. Whatever our readers (e.g. peer reviewers, other scholars) demand is what counts as the necessary data and methods details. This scholarly author-reader relationship is very different from the novel author-reader relationship. In our case, by making publiclly available our data and our code at the point of publication of our articles, we make it a lot easier for our readers to find the materials they need to evaluate our claims and resuse our work. Many fields are shifting their scholarly practices in recognition of this special relationship between author and reader. A full account is beyond the scope of this chapter, but betailed desriptions of these new ways of doing science are available for Oceanography (Lowndes et al. 2017) and soil science (Bond-Lamberty, Smith, and Bailey 2016). Returning to the question of computational reprodubility, my view is one of the most pressing problems is that our research publications rarely include enough of the data to enable exact reanalysis, and our methods are rarely presented in enough detail to allow another person to independantly repeat our analysis unabimguously. This type of reproducibility is becoming an important issue for recent archaeological research due to advances in technology and the rapid spread of computational methods for many kinds of basic archaeological analysis. This can make it difficult to trust published research, and difficult for others to use newly published research in their own work. The short answer to improving computational and statistical reproducibility in archaeology is for archaeologists to include data files and programming code with their publications. These extra documents will allow another researcher to see all the steps from when the raw data entered the researcher’s computer to the end point where these data were summarised and visualised in a journal article. Literate programming methods are an efficient solution to this problem of combining code with the narratice text of publcations and reports. 3.3 Literate programming The general principle of literate programming is to write code and text in one document so that a human reader can easily understand the purpose of the code (Knuth 1992). The original context of literate programming was to have a system for writing code so that text can be included among the code to explain the algorithms and data structures to other human readers. The concept of literate programming has evolved over time, and taken on a slightly different meaning within the R community, which might be more accurately called ‘literate computing’, ‘literate statistical programming’, ‘literate data analysis’, or ‘literate statistical practice’ (Schulte et al. 2012; Rossini, Lumley, and Leisch 2003; Koenker and Zeileis 2009; Gentleman and Temple Lang 2007; Hoefling and Rossini 2014). These new terms reflects a shift in the focus of literate programming from what was originally writing computer programs with some annotations for human users, to writing research reports that contain programming code to do data anlaysis and visualisation. Although the terminology is varied, the essence of the concept is that programing code is embedded or interleaved with the narrative text in the same document. Writing research documents using a literate programming method improves reproducibility because a reader can see the details of the analytical methods used to compute the statistics, tables and plots of the publication or report. We do not need to copy and paste results from one program to another, because the code produces the results in situ each time the document is rendered. This elimintes errors from copy-pasting into a Microsoft Word document, where we have the risk that we might accidently skip some parts of the text that need updating. This approach also improves efficiency because we have the code and text in one place, greatly simplifying the organisation of our files. This makes it easier for us to navigate our project directories, and makes it easier to share files with collaborators and other people because they can more quicky find their way around. Using R we have several options for doing literate programming. The one I recommend is centred on Yihui Xie’s knitr package (plus several others) and the markdown plain text format. It is easy to learn, very well supported with extensive documentation (Xie 2015), and enables all the usual scholarly writing requirements, such as captioning and cross-references figures and tables, citations and complex notation. The file format we use for this is called R Markdown, which is a plain text file that has the .Rmd suffx. We write text and code in the Rmd file using a text editor (such as RStudio), and ‘render’ or ‘knit’ that Rmd into an output format such as Microsoft Word, HTML or PDF. The rendered version contains the formatted text, citations, tables, figures, etc., and is suitable for submission to a journal, or circulation to colleagues. When writing an Rmd file, you typically have three key components: (1) the document metadata (usually a section at the top of the document), (2) the narrative text (the text of your article or report), and (3) the code blocks (sections of R code in between paragraphs of narrative text that generate plots, tables and other output to appear in the rendered document). 3.3.1 Document metadata, citations and the bibliography The metadata section is defined by three dashes at the top and bottom of the section. In this section, we can indicate the title, author, date and many other details of the document. Crucially we can also specify what type of document to render the Rmd into, e.g. Microsoft Word document, PDF or HTML file. The metadata contains instructions for document-level details such as whether or not to include a table of contents, what style to use for the citations and reference list, and what template should be used to style the rendered document (e.g. to add line numbers). Here is an example of the metadata section from the Rmd file of one of my publications: --- title: &#39;Movement of lithics by trampling: An experiment in the Madjedbebe sediments, northern Australia&#39; author: - Ben Marwick - Elspeth Hayes - Chris Clarkson - Richard Fullagar date: &#39;2019-05-28&#39; output: bookdown::word_document2: fig_caption: yes reference_docx: templates/template.docx bibliography: trampling.bib csl: journal-of-archaeological-science.csl abstract: | Understanding post-depositional movement of artefacts is vital to making reliable claims about the formation of archaeological deposits. Human trampling has long been recognised as a contributor to post-depositional artefact displacement. We investigate the degree to which artefact form (shape-and-size) attributes can predict how an artefact is moved by trampling. We use the Zingg classification system to describe artefact form. Our trampling substrate is the recently excavated archaeological deposits from Madjedbebe, northern Australia. Madjedbebe is an important site because it contains early evidence of human activity in Australia. The age of artefacts at Madjedbebe is contentious because of the possibility of artefacts moving due to trampling. We trampled artefacts in Madjedbebe sediments and measured their displacement, as well as modelling the movement of artefacts by computer simulation. Artefact elongation is a significant predictor of horizontal distance moved by trampling, and length, width, thickness and volume are significant predictors of the vertical distance. The explanatory power of these artefact variables is small, indicating that many other factors are also important in determining how an artefact moves during trampling. Our experiment indicates that trampling has not contributed to extensive downward displacement of artefacts at Madjedbebe. keywords: | Trampling; Artifact movement; Experimental archaeology; Australia; Simulation --- The language of this metadata is not R, but a format called YAML, and is distinctive with its key: value patterns. This example shows just a few of the possible metadata fields. The title, author and abstract fields are straightforward, but you can see that the date field has some R code that will insert the current date when the document is rendered. This means that the outputted docx file will automatically have the date of the latest change, and I don’t have to worry about updating it by hand. The output: field shows that I have used the word_document2() function from the bookdown package to render this Rmd into a Microsoft Word document. This word_document2() function is important for writing scholarly documents with R Markdown because it provides the automatic numbering and cross-referencing of figures and tables in the document. The bookdown package also contains related functions for writing a multi-part document such as a book or manual. In the document metadata above I have also indicated that I want to show the figure captions in the output doucment with fig_caption: yes. The line reference_docx: templates/template.docx indicates that I am using a docx file called template.docx as custom template to add some styling to my output. In this specific case, the template.docx adds line numbers to the rendered docx file, specifies Times New Roman as the body text font, and a few other minor adjustments to meet the submission requirements of our target journal. The rticles package by RStudio provdes Rmd templates for manuscripts for several major scholarly publishers. A template for producing output that follows the APA style (American Psychological Association) can be found in the crsh/papaja package. Other templates are available in the the [Pakillo/rmdTemplates/])(https://github.com/Pakillo/rmdTemplates/) package. Many Rmd metadata fields are documented at http://rmarkdown.rstudio.com, and https://www.rstudio.org/links/r_markdown_reference_guide. I have also found many useful examples of Rmd metadata by browsing and searching github.com. We can also see the file containing the bibliographical information specified at bibliography:, and a file that gives the in-text citation and reference list style at csl: (citation style language). R Markdown can handle bibliography files in a variety of formats, including RIS, EndNote’s .enl and BibTeX’s .bib. I typically use BibTex, and my .bib file is a plain text file that I create by copying and pasting the reference details from various sources, such as my reference manager library (e.g. EndNote, Mendeley, Zotero) or directly from Google Scholar. The .csl file is one I copied directly from a collection of journal citation styles (e.g. from http://citationstyles.org/). Here are the bibliographic details for one of the items in the .bib file for this book: @article{Marwick2016JAMT, year={2016}, issn={1072-5369}, journal={Journal of Archaeological Method and Theory}, doi={10.1007/s10816-015-9272-9}, title={Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation}, url={http://dx.doi.org/10.1007/s10816-015-9272-9}, publisher={Springer US}, keywords={Reproducible research; Computer programming; Software engineering; Australian archaeology; Open science}, author={Marwick, Ben}, pages={1-27}, language={English} } Most of the lines in the above example are easy to understand attributes of a bibliographic reference. Perhaps the most important detail is in the first line, Marwick2016JAMT, which is the key that we used to link the in-text citation to these bibliographic details. To include a citation of this paper in my R markdown file, I type [@Marwick2016JAMT], and this is rendered in the output document as a nicely formatted citation like this: (Marwick 2016). All of the usual variations of in-text citations are possible with this notation: Effect Markdown notation Result Complex combinations of citations ...some narrative text... [see @Marwick2016JAMT, pp. 13-15; also @knuth1992literate, ch. 1]. …some narrative text… (see Marwick 2016, 13–15; also Knuth 1992, ch. 1). Page ranges ...some narrative text... [@Marwick2016JAMT, pp. 13-15, 18-19 and elsewhere]. …some narrative text… (Marwick 2016, 13–15, 18–19 and elsewhere). Multiple citations ...some narrative text... [@Marwick2016JAMT; @knuth1992literate]. …some narrative text… (Marwick 2016; Knuth 1992). Date-only citation Marwick says you should use a scripting language for your analysis [-@Marwick2016JAMT]. Marwick says you should use a scripting language for your analysis (2016) Use author’s name in text @Marwick2016JAMT says you should use a scripting language for your analysis. Marwick (2016) says you should use a scripting language for your analysis. Use author’s name and page number @Marwick2016JAMT [p. 13] ays you should use a scripting language for your analysis. Marwick (2016, 13) says you should use a scripting language for your analysis Multiple cites by same author many good examples by Marwick et al. [-@MarwickVanVlack; -@MarwickJerimalai] many good examples by Marwick et al. (2017, 2016) When the Rmd document is rendered, these citations appear in the output document with proper styling and the reference list is automatically generated and added to the end of the document. The styleing of the citations and reference list is controlled by the .csl file, it could be author-date, as above, or numbered citations, etc. Typically, directly after this block of metadata we have a block of code that contains all the library() functions to enable all the functions that we use from contributed packages. This is a useful convention because it helps other readers quickly see what packages are required to perform the analysis in your report. In this book I have taken a different approach and show the library calls much more often than strictly necessary to help you make associations between libraries and the functions they contain. 3.3.2 Narrative text, structuring the document and special characters in the text Narrative text refers to the body text of your document, such as this paragraph you’re currently reading. Most archaeologists are accustomed to writing in a word processor such as Miscroft Word, where can change the appearance of the document while we write, for example by adding bold formatting and changing the size of the font. When we use Microsoft Word, we are using a “what you see is what you get” editor – we edit a document in a form resembling the final printed or presented document. Writing an R Markdown document is a different experience because we writing in plain text, and we include instructions in the document that only apply formatting during the process of rendering to generate the output. This means that while we are writing an R Markdown document we do not see bold text or text in difference sizes. This is a very typical writing experience for many researchers in the natural and physical sciences, where the LaTeX document preparation system is popular for writing journal articles (Brischoux and Legagneux 2009). LaTeX is a widely used code-based document preparation system, with particular strengths in complex formatting needed by technical scientific documents. Although highly flexible, LaTeX is also complex and verbose, requiring a lot of typing to add the formatting code to your document. LaTeX requires a basic working knowledge of computer programming, so non-programming researchers seldom use it. This method of creating documents is preferred in disciplines such as mathematics and physics, where complex equations are written using code, because for many people it is easier to write code to produce equations than use a mouse to click on menus and buttons to select the special characters needed for equations. Markdown is closely related to LaTeX, but much simpler system for preparing documents. Markdown does not require any programming skills to use, so it is well-suited for researchers in the social sciences. The advantage of Markdown over LaTeX is that the document is much less cluttered with formatting code for typical writing, but LaTeX can be used where more complex symbols, expressions or formatting are necessary. There are several reasons why a plain text document preparation system might be prefereble over a word processor. The most compelling one is that we cannot interweave exectutible R code in a word processor document. Of course we can add R code to a word processor document, but there is no way to run the code in that document to update the document. We must copy and paste updated results into the word processor document. With a plain text format such as R Markdown, we can include R code, and have that code generate output to update the figures, tables and other results in the document. This is the most important charactersting of R Markdown that makes our documents more reproducbile. Finally, plain text files are able to take advantage of the git version control system for collaboration. This system allow fine-grained control of the history of changes to a plain text document, more on this below in the Version Control section below. Some other reasons to use a plain text format that are important to me include portability, compactness, security and collaboration (Cottrell 1999). Portability means that when we use a plain text system such as R Markdown, anybody, using any computer platform, will be able to read our text, even if they don’t have the means to view or print the rendered version. With plain text, the accessibiltiy of the contents of our document is not restricted to a certain program, or a specific operating system, which might become increasly hard to find working copies of in the future. This guarantee of long-term accessibility should have a special appeal to archaeologists who are more senstive than most researchers to the decadal pace of technological change. Compactness refers to the size of the file, often plain text files requires orders of magnitude less disk space that corresponding word processor files. This means plain text files are more quicker and more responsive to work with, and easier to circulate and store. Security is about the risk of losing your document to software crashes or file corruption. With a plain text document we have much less that can go wrong in the normal course of preparing the document. Choosing software to write plain text documents can be challenging because of the wide variety of options. I recommend RStudio because it contains comprensive support for writing R Markdown, saving time and effort. For example, RStudio will highlight markdown text and R code in a way that makes it more readable. It will automatically indent or tidy up your code as you write it. It will also passively signal to you when you’ve done something wrong in your code (such as forget a closing bracket, semicolon or quotation mark). RStudio also comes with Pandoc built-in, which is the software that helps with converting between Markdown and Micrsoft Word, PDF and HTML. Having Pandoc built-in to RStudio saves you from working with Pandoc outside of RStudio in a terminal. Other popular editors for writing R Markdown documents include Sublime Text (for OSX only), Notepad++ (for Windows only), Atom (cross-platorm) and Emacs (also cross-platorm). These other editors are ideal if you enjoy spening time configuring and customising your writing environment. If your priority is writing text and code, or you are collaborationg with authors who don’t enjoy spening time configuring and customising, then RStudio is a good choice. The RStudio text editor, like the others, do not include any tools for formatting your text in situ, for example you cannot make words appear in the editor with bold or italic characters. This is a deliberate design, and is typical among text editing programs used for writing code and Markdown. When working with a plain text editor, we type unformatted plain text and code, and among the text we include instructions on how the final document should appear. For example if we want a word to appear in italics, we would type in our R Markdown document it like *this*. In this example, when we convert our R Markdown document into a final version, such as a PDF, the two asterix symbols will be interpreted by Pandoc to produce a document with this word in italics. Headings and subheadings are indicated by # symbols, the top level heading has a single # directly preceeding the header text, and second and third level headings have ## and ###, and so on, like so: # This is my top-level heading, such as &#39;Introduction&#39; ## This is my second-level heading, suitable for a sub-heading ### This is my third-level heading, i.e. a sub-sub-heading. Probably we don&#39;t want to many levels of heading below this one because it may overwhelm our reader. Typically in writing for research we need to include special symbols that are not part of the common character set. The following table shows how to generate commonly used symbols in a R Markdown document, the notation surrounded by dollar signs $ is LaTeX notation: Symbol Markdown/LaTeX notation per mille ‰ $\\text{\\textperthousand}$ delta δ $\\delta$ plus-minus ± $\\pm$ degree ° $\\text{\\textdegree}$ subscript CO2 CO~2~ superscript 14C ^14^C With LaTeX it is possible to include a great range of mathematical symbols into your text. There are numerous online sources to look up the notation for a specific symbol or expression, e.g. https://en.wikibooks.org/wiki/LaTeX/Mathematics 3.3.3 Code blocks In an R Mardowm document code blocks (also known as chunks) are demarcated by three backticks, followed by curly brackets indicating the programming language of the code. Here is a very simple code block that demonstrates how the backticks are used: ```{r} 1 + 1 ``` If this block was in our document, the output would look like this: 1 + 1 ## [1] 2 Often in writing for research we don’t want all the code blocks to be visible in the rendered output because they disrupt the reader’s experience with the contents of the document (and journal editors don’t like to see code blocks in submitted manuscripts). We can stop the code blocks from appearing in the rendered output in two ways. First is an option we can set for each block in the document, in case we case to show some blocks and hide others in the rendered output. Second is a single setting we can make for the whole document that hides all the blocks. To hide one code block we can use the echo option in the block we want to hide, like this: ```{r echo = FALSE} 1 + 1 ``` But to hide all the blocks, we add a new block at the top of our document, directly below our document metadata, to set document-wide options, like this: ```{r include = FALSE} library(knitr) opts_chunk$set(echo = FALSE) ``` Note that this example has both include =, and echo =. They are subtly different, include = FALSE will let the code run, but include neither the text of the code nor its output in the rendered document, while echo = FALSE hides the code but lets the output appear (such as a plot). There are a large number of options for controlling the behaviour of code blocks in R Markdown documents. These are documented at https://yihui.name/knitr/options/, Xie (2015) and Gandrud (2015). I typically use these options to configure how the code blocks appear in my rendered documents: library(knitr) opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 900, fig.align = &#39;center&#39;) # image width should be 90 mm for 1 col, 140 mm for 1.5 col, 190mm for two cols fig_width_two_col &lt;- 190/25 # inches fig_width_one_col &lt;- 90/25 # inches In addition to hiding all the code blocks with echo = FALSE, the warning = and message = options prevent these types of messages from appearing with my output. This means that if I have a code block that generates a plot, but might also produce a warning, I will not see that that warning in my rendered output. I will still see the warnings and messages while I am developing the code interactively in the console, and they will be printed in the console when the document is rendered. The dpi = option ensures that the resolution of the plots that are created by the code blocks are suitably high, and fig.align = centre-aligns my figures. The two lines at the bottom of that code block create objects that store a value that I use in other code blocks to set the width of the plot or figure that a code block produces, more on that below. Thre are many other possible options that can be set at this point, to see the full list, run ?opts_chunk in your R console, and see https://yihui.name/knitr/options/#chunk_options for a description of what they do. 3.3.3.1 Saving time with blocks of long-running code If you have a block of code in your document that takes some time to run (e.g. more than 5 minutes), there are several options to avoid having to run that code every time you render the document. One simple method is to using caching for the code block, so that when the code block is first run, the results will be stored. These stored results will be each time the document is rendered, saving time waiting for the code to run again. The cached results will be reused until the code in that code block is changed. After the code in that block is changed, the code will be re-run then next tim that the document is rendered. Here’s an exampe of a code block that uses caching: ```{r code-block-A, echo = FALSE, cache = TRUE} z &lt;- rnorm(10) # generate ten random numbers ``` Because we set cache = TRUE, the object z will be stored and reused each time we render the document, until we make a change to the code in this block. Sometimes we might have a mix of cached and unchached code blocks, and some of the cached code blocks depend on each other. For example, suppose we have code block A (cf. the code block above) that performs a long-running computation, and later in our document we have code block B that uses the result of A to perform a second, different long-running computation. We would use cache = TRUE for both code block A and B to store the results so we don’t have to wait while we work on other parts of the document. But we also need to indicate that if there is a change to the code in code block A, but no change in the code in code block B, then the code in code block B still needs to be re-run to incorporate the update in code block A. We can manage this dependency between these two code blocks by adding dependson = to code block B, for example: ```{r code-block-B, echo = FALSE, cache = TRUE, dependson = &#39;code-block-A&#39;} mean(z) ``` We can also use dependson = to reference multiple code blocks, for example dependson = c('code-block-A', 'code-block-C'). The use of dependson = in code block in an R Markdown document ensures that when other code blocks are changed, this block will be updated accordingly. Caching can increase the complexity of a document substantially, and I generally use it sparingly. Further readering on the details of caching in R Markdown can be found at https://yihui.name/knitr/demo/cache/. A second strategy for saving time with long-running code is to explicitly save the output on the first run, and then re-use that output by loading it in later code blocks. A typical use-case for this method is when a computation takes days or weeks, for example, on a high-perfomance computing facility, and you want to save the output in a portable format and reuse it in different documents. In this case you might save the output as a plain text open format file (such as CSV or XML) and load them with a read_xxx function (more on these in Chapter X). Or you might save the output of the long-running computation as an RData object using save() (ideal for when you want to save many objects) or save as a rds object using saveRDS() (ideal from when you want to save a single object, such as the object z in the code block above). Although plain text open file formats are better for long-term presernation of data, an RData object might be preferable if the structure of the output data if difficult to represent in a flat table (for example, a deeply nested list). To use a saved data object we can include in a code block load() for RData files, and readRDS() for rds files. 3.3.4 Cross-referencing and captioning figures Another important option for managing code blocks in an R Markdown document is labelling them. Labelling is useful for two reasons, firstly for efficiently navigating through many blocks in a document, and secondly to enable cross-referencing of tables and figures. Here’s an example of a code block with a label: ```{r simple-addition, echo = FALSE} 1 + 1 ``` In this example the label is ‘simple-addition’, and this is the key to make cross-references work in R Markdown. Code block labels do have a few constraints, they can only contain alphanumeric characters (a-z, A-Z, 0-9), slashes (/), or dashes (-). Underscores are not allowed in code block labels, and code block labels must be unique, you cannot have two blocks with the same label. We use the labels to link in-text cross-reference to the output of the code block with that label. The general form of cross-referencing is to type \\@ref(fig:label) where label is the code block label that you are referring to. If you are referring to a code block that produces a table (by using knitr::kable()), then the general form is \\@ref(tab:label). In addition to a code block label, a second element is required to enable cross-referencing. This is a caption, which we can add using the fig.cap option in the first line of the code block. For example, this code block produces a simple plot, and is labelled ‘simple-plot’, and we have set the caption as ‘This is a simple plot’ : ```{r simple-plot, echo = FALSE, fig.cap = &quot;This is a simple plot.&quot;} plot(1:10) ``` plot(1:10) Figure 3.1: This is a simple plot. When we want to refer to that plot in our text, we type \\@ref(fig:simple-plot) and this will be rendered into the figure number that this plot has in our document, for example, Figure 3.1. Occasionally we need to include a cross-reference within the caption of a table or figure. To make this work need to add one backslash, like this \\\\@ref(fig:simple-plot). The second backslash ‘escapes’ or hides the other backslash from being interpreted by R. This is necessary because the backslash is one of a small set of special characters, or metacharacters that have specific meaning in R that is different from how we use them in English writing. The others metacharacters are: $ * + ? ^ { } | ( ) and the period at the end of this sentence. The special properties of these metacharacters are very useful for cleaning and manipulating data, we’ll discuss this more in Chapter XX. Relating to figures, another code block option that I use often is fig.width. When writing a manuscript for submission to a journal, I will check the journal’s instructions to authors for information about figure sizes. Often they specify a width in millimeters for images that span one or two columns. You can see in one of the above code blocks I created an object called fig_width_one_col, this just holds a single numeric value for the width of a figure (in inches) in order to span two columns in one of the archaeology journals that I have published in. This saves me time so I don’t have to adjust the width of each figure indvidually. In this example we can see three important functions: 1) how to set the width of the plot using fig.width =, 2) how to cross-reference another figure elswhere in the document with \\\\@ref, and 3) how to use a citation in a caption (just like we would in a paragraph of narrative text: ```{r simple-plot-one-col-wide, echo = TRUE, fig.cap = &quot;A simple plot. This plot is one column wide, unlike Figure `` `\\\\@ref(fig:simple-plot)` `` which is wider. A classic source of information for plotting in R is @murrell2016Rgraphics.&quot;, fig.width = fig_width_one_col} plot(1:10) ``` Figure 3.2: A simple plot. This plot is one column wide, unlike Figure 3.1 which is wider. Good sources of information for plotting in R include Murrell (2016) and Wickham (2016). 3.3.5 Cross-referencing captioning tables Including tables in R Markdown is similarly straightforward. I typically use the kable() function from the knitr package, and occassionaly Hao Zhu’s kableExtra package for more elaborate tables. However, there are a many other packages for producing more complex and flexible tables. There is an informative table comparing some of the more widely used packages in the vignette of the huxtable package (https://cran.r-project.org/web/packages/huxtable/vignettes/design-principles.html). A detailed review of all the options for making tables is beyond the scope of this chapter, and we can simply note here that you can make any table design you want in R Markdown, with a little effort. Table 3.1 shows simple table that results from the kable() function from the knitr package: Table 3.1: Rule Details O Order rows and columns by size or some other meaningful measure R Round to two or three significant or effective digits. A Average or sum rows and columns to provide a visual focus C Columnize: put items to be compared in the same column, one above the other. L Lay out the table to facilitate comparisons, don’t insert too much white space: things to be compared should be close to each other, but add gaps every 5 or so rows to help the eye travel across the table. E Explain the patterns of the data and use descriptive titles Caption with ‘invalid multibyte string’: “The table summarises some simple and classic guidelines for presenting data in tables (Ehrenberg 1977; Feinberg and Wainer 2011; Wainer 1997). To make the guidelines more memorable, we can abbreviate them into the mnemonic ‘oracle’. These guidelines should not be followed blindly: tables are meant for communication, and you should rather ignore these guidelines than slavishly follow them and corrupt the message of your table.” Notice that the method for adding a caption to a table is slightly different from captioning a figure. For a table, we add a caption with the caption = argument in the kable() function, rather than the fig.cap = option for the code block. However, the method for cross-referencing a table is similar to cross-referencing a figure, we type \\@ref(tab:simple-table), which automatically gives us the table number when rendered: 3.1. Just like when referencing figures we use the code block label as the link. The only difference in the cross-reference syntax is tab: for tables instead of fig: for figures. A key principle to keep in mind when preparing a table for a report or publication is that a table is for communication, not data storage. To store data for resuse you need to archive a digital file, saved in an open, plain text format (such as CSV), and deposit it in a well-known and trustworthy data repository, and include in your publication a persistant URL (such as a DOI) to the file. Tables interleaved amongst text are meant for human eyes and minds, not for machine ingest, so you should design them with this in mind to opitmise their effectiveness. In any cases a plot many be more effective that a table, see Gelman, Pasarica, and Dodhia (2002) for examples of plots can improve on tables. 3.3.6 Using inline R code to put results in the text Often in writing for research we want to include some numeric output within our text, such as the output of some data analysis or a hypothesis test. For writers using Microsoft Word or similiar software, they will compute their result in another program, such as Microsoft Excel or SPSS, and then paste the results into their Word document. The problem with this method is keeping the Word document up to date during the many iternations that data anlyses often involve. There is a mental burden to keeping track of all the locations in a document where a value needs to be updated when a new analysis is completed. R Markdown provides a solution to the problem with in-line R code. When writing in R Markdown document, we can write R code in the middle of a sentence to compute a result, and when the document is rendered, the code is excecuted and numerical output appears in our rendered document in place of the R code. When typing in-line R code we demarcate it with a single backtick at the start and end, and the code is prefixed with the letter ‘r’ to indicate the programming language. Here is a simple example: in our R Markdown document we type `r 5 * 2` (as in ‘five times two’), and when we render that document we will see 10 in place of the R code. When writing in-line R code it is good practice to keep the code short so that the code doesn’t interrupt the flow of the sentence for the reader (i.e. you, when you are writing the Rmd document, and your colleagues who are also editing that document). An easy way to keep the in-line code short is to use a code block for the more complex operations, and then assign the result that you want to show in the text to an object. You then use that object in the in-line R code to make the values appear in the rendered text. Here is a simple code block to demonstrate this approach: # We add up all the numbers from 1 to 10 x &lt;- sum(1:10) And here is our narrative text that includes inline code to use the output from that code block in a sentence. The inline code is typed as: `r x`, and we can use the output like this: The sum of all the numbers from 1 to 10 is 55. This appoach is especially convienent for extracting a few values value from a large table or other data object, and rounding them so it is suitable to include in a sentence. A common use-case for this approach is when we do a statistical hypothesis test and want to report the test statistic and p-value in parentheses in of a sentence. More on this in Chapter XX. For further reading on more details on figures, tables, cross-referencing and citation in R Markdown documents I recommend Xie (2016). For more details on customising code chunks, a good reference is Xie (2015). 3.3.7 Rendering the R Markdown into an output file By far the simplest method to render, or convert, an R Markdown document into a Microsoft Word, HTML or PDF document is to click on the ‘Knit’ button at the top of the source editor window in RStudio. The Knit button will inspect the settings in our R Markdown document metadata, and render the output according to those specifications. It is of course also possible to use a function to render the document from the console. For example rmarkdown::render(&quot;my_document.Rmd&quot;) (i.e. the render function in the rmarkdown package) or knitr::knit(&quot;my_document.Rmd&quot;) (i.e. the knit function in the knitr package). These functions can take many options to control how the Rmd file is rendered, however, I generally avoid using them because they are bad for reproducibility. Because a document cannot render itself, we must use the knit or render functions outside of the document, and this means that some of the options for how the document is rendered might not be contained in the document. I prefer to have the rendering options self-contained in the Rmd document, and then either use the knit button, or a very simple function such as render(&quot;my_document.Rmd&quot;) with no additional arguments. This means that I will always have a record of the settings I need to render the my document because they are contained within the document itself, rather than as arguments to the render() function. Generally I eschew mouse-driven actions in the course of data anlaysis, because they often leave no trace and are bad for reproducbility. The two major exceptions are rendering an R Markdown into an output document, where I recommend a mouse-click on the knit button in RStudio, and setting the working directory. For setting the working directory, I recommend using the RStudio toolbar and clicking on ‘Session’ -&gt; ‘Set Working Directory’, and then usually the best option is to choose ‘To Source File Location’. However, when you become more familiar with complex file structures you may find the other options more useful. By excluding these two actions from the code in your R Markdown document you make your document more portable. This is because the full path of the working directory your computer is probably unique to that computer, and if another person attempts to use your code on their computer any function that uses your path will fail because your path does not exist on their computer. For rendering the document, it is important to keep the rendering options specified in the document metadata (i.e. the YAML front matter at the top of the Rmd file), rather than in a render() function call. This ensures that when another person attempts to render your document, then have all the information necessary about, for example, what .bib file to use for your citations and references. 3.4 Summary In this chapter I described a key principles of reproducible research, literate programming. This is dervied from computer science research and has been adopted by other disciplines, such as ecology and political science, to enable computational and statistical reproducibility. I also demonstrate the current software tools (R Markdown and knitr) that allow us to put this principle into practice. Over time, these software tools will change, and perhaps be replaced by something completely different. However, the principles that they are based on will endure, and are worth becoming familiar with as key concepts in computational and statistical reproducibility. References "],
["dataorganisation.html", "Chapter 4 Collecting &amp; organising data to analyse with R 4.1 Overview 4.2 File organisation 4.3 Spreadsheets organisation 4.4 Cell organisation 4.5 Summary", " Chapter 4 Collecting &amp; organising data to analyse with R 4.1 Overview The purpose of this chapter is to explain how thoughtful decisions made before and during the point of data collection can save time and effort later during the data analysis and visualisation process, using R or any other tool. The chapter is organised hierarchically, starting with file oranisation at the project level, then moving down to discuss efficient organisation of spreadsheets (as one of the most common tools for collecting, sharing and storing archaeological data), and finally at the highly granular level of the individual cell within a spreadsheet. Saving time and effort is of course an important motivation for changing your data collection and analysis behaviours, however there is a greater vision that relates to data organisation principles. This vision is two-fold. First is enhancing the reproducibility of archaeological research. When a project is well-organised, it is easier to find the key files to check the previous results. Second is large-scale inter-operability of datasets, so that archaeological data from diverse projects might be easily combined to address questions that no single project can by itself. This vision has many other dependencies, such as a willingness to share data and make it publicly available, and agreement about what to measure and how to name variables. While sharing your data is the first step in allowing reuse, and this is not yet common in archaeology, to ensure that it is easy for others to reuse your data they must be easy to understand (White et al. 2013). Much of the content of this chapter is not highly sophisticated, and might seem like common sense. However, because the practices decsribed here make a big difference to the reusability of data, I think it is important to discuss these issues directly and explicitly as part of learning about the data analysis process, rather than assume archaeologists will ‘pick them up’ along the way without any specific guidance or instruction. This chapter draws on materials in the public domain that were originally prepared by Jenny Bryan, Karl Broman, and others for Data Carpentry’s reproducible science curriculum as well as sources from other disciplines such as Borer et al. (2009), Sutter et al. (2015), Volk, Lucero, and Barnas (2014), Hampton et al. (2013), Strasser and Hampton (2012). 4.2 File organisation 4.2.1 Guidelines for organising your files for improved reproducibility The most fundamental principle to follow to improve file organisation is simply to have a strategy and be faithful to it. A key motivation for organising files is to make it easy to work with them, both during the time of your anlaysis, and in the future when you or other people use the files. If your files are well-organised, someone unfamiliar with your project (which could also be you in the future) should be able to look at your files and quickly understand in detail what you did and why (Noble 2009). A second motivation for being organised is that for almost everything you do during your analysis, you will probably have to do it again. We commonly explore many analytical options with our data, so the process of data analysis is more like a branching tree than a single sequence of consecutive actions. Often we loop back to an earlier stage in the analysis to explore the effect of adjusting a selection of data, or the parameters of a model. This means that many parts of our analysese are redone in the natural flow of working on a project. If we have organized and documented our work clearly, then repeating the earlier steps of the analysis with minor variations will be much easier. Beyond the fundamental principle of simply having a strategy, there are several guidelines that have been proposed by biologists and ecologists that are applicable to archaeological research (Noble 2009; White et al. 2013). These guidelines relfect the principle that file organization should indicate inputs and outputs and the flow of information during your data anlysis. The most important of these are that (1) raw data is kept separate from derived data, and (2) data is kept separate from code. Raw data means data files as they were when you originally received them, for example transcribed from a notebook, directly entered into a database or spreadsheet, or downloaded from an instrument such as a total station or XRF. Derived data refers to the data that result from actions you take after you receive the data files. This includes manual manipulations, such as edits in an Excel spreadsheet, and programmatic manipulations that occur when you use R code to output an updated version of your data. Manual manipulations should be avoided as much as possible because they leave no trace, unlike a manipulation performed by code, making them difficult to undo if you change your mind. If a manual manipulations are necessary in your data, you should keep a plain text file with the data file and type in that file a brief narrative of the changes you make so that you have a record of them. A simple way to keep raw data separate from derived data is to have a folder (also known as a directory) called data/ that contains two directories, raw_data/ and clean_data/, see the schematic below. Using a simple structure like means you only need one copy of the original raw data files in your project files, rather than duplicating them across your file structure, which can be a source of substantial confusion about which file is the correct one. |- data # raw and primary data, are not changed once created | |- raw_data/ # raw data files, will not be altered ever | +- clean_data/ # cleaned data files, will not be altered once created This practice of keeping the raw data isolated from everything else means that you are always able to loop back to the first steps of your anaysis where you read in the raw data in case you need to explore a different path or check your results. This is vital for ensuring the reproducibility of your research, and for peace of mind that you can check your results. If you make many changes to your raw data files along the way of your analysis, you may never be able to retrace your steps back to the start of your analysis, you will not have the option of checking your results, and your work will be irreproducible. Keeping the raw data intact also gives other researchers more options when they reuse your data, increasing the value of your work to the broader research community of archaeologists. The second file organisation guidelines we can take from biologists and ecologists is closely related: data is kept separate from code. When working with an Excel file this is a very unnatural way to work. In an Excel sheet, the formulas that compute on the data are located directly adjacent to the cells that contains the data (e.g. in the rows to the right or below the data). While this may be convienent for working quickly with small tables of data, it results in a workflow where it is difficult distinguish between raw and derived data, and between data and methods (i.e. functions in speadsheet cells). This mingling of data and method is a major impediment to reproducbility and openness because the sequence of decisions made about how to analyse and visualise the data is not explicit. Your code will change frequently while you work on an analysis, but your raw data should not change. If you keep these files in separate folders you will be less tempted to change the raw data by hand while you are writing code. This is a core principle of software design, and helps to minimise confusion when browsing the files in a project. A second reason why it is good practice to keep code separete from data is that you may be unable to share your raw data (for example, it may contain senstive information about archaeological site locations), but you may be willing to share your code. When these two compoenents of the project are well-separated, it is easy to control what can be made public and what must remain private. Here is an example of a project folder structure that shows how code and data can be seperated: |- data/ # raw and primary data, are not changed once created | |- raw_data/ # raw data files, will not be altered ever | +- clean_data/ # cleaned data files, will not be altered once created | |- code/ # any programmatic code, e.g. R script files A third guideline relating to the general idea of separation is to keep the files associated with mansuscript or report production separate from everything else. In the example below this is represented by the paper/ directory. It contains one (or more) R Markdown files that document the analysis in executable form. This means that it should be possible to render the R Markdown document into a HTML/PDF/Word file that shows the main methods, decisions and results of the analysis. This R Markdown file might connection to the code in the code/ directory by using the source function to import code from a script in code/ into the R Markdown file. This example also shows how files for tables, figures and other images can be organised: |- data/ # raw and primary data, are not changed once created | |- raw_data/ # raw data files, will not be altered ever | +- clean_data/ # cleaned data files, will not be altered once created | |- code/ # any programmatic code, e.g. R script files | |- paper/ # all output from workflows and analyses | | report.Rmd # One or more R Markdown files | |- tables/ # text version of tables to be rendered with kable in R | |- figures/ # plots, likely designated for manuscript figures | +- pictures/ # diagrams, images, and other non-graph graphics A final guideline related to the general theme of separation is to have a ‘scratch’ directory for experimentation. This directory should hold snippets of code and output that are produced during short journeys down alternative paths of the analysis, when you want to explore some ideas, but you are not sure if they will be relevant to the main analysis. If the experimental analyisis proved worthwhile, you can then incroporate it into the appropriate place in the main project folders. If the experiment is not directly relevant to the main analysis, you canleave it in the scratch directory or delete it. The most important details about the scratch directory is that everything in the this directory may be deleted at any time without any negative consequences for your project. I typically delete my scratch folder after my paper is submitted or published. A final guideline, not related to separation, is to include a plain text file named ‘README’ (e.g. “README.txt” or “README.md”) at the top level of a project file structure to document some basic details about the project. This might include about dozen lines of text providing the project name, the names of the people involved, a brief description of the project, and a brief summary of the contents of the folders in the project. You may also wish to include similar README files in other folders in your project, for example to document instrument parameters that are important for using the raw data. Below is an example of a project file structure that includes a scratch directory and a README file: |- data/ # raw and primary data, are not changed once created | |- raw_data/ # raw data files, will not be altered ever | +- clean_data/ # cleaned data files, will not be altered once created | |- code/ # any programmatic code, e.g. R script files | |- paper/ # all output from workflows and analyses | | report.Rmd # One or more R Markdown files | |- tables/ # text version of tables to be rendered with kable in R | |- figures/ # plots, likely designated for manuscript figures | +- pictures/ # diagrams, images, and other non-graph graphics | |- scratch/ # temporary files that can be safely deleted | |- README.txt # the top level description of he project &amp; its content These guidelines about separation are broadly applicable to most archaeology projects, regardless of scale or methods. Some adjustments of the details provided in the schematic above may be required for large or secure datasets that cannot be stored on a laptop, and for time-consuming computations. In any case, the general principle of separation remains a useful starting point for organising files. 4.2.2 Guidelines for naming your files to improve reproducibility As archaeologists we are accustomed the challenge of naming things, when working with finds that don’t fit neatly in existing typologies, or identifying fragments of objects that lack distinctive attribute [Kleindienst (2006); Ferris (1999). We often take care when naming things because names are communication, and privilige some interpretations and meanings over others. Often as archaeologists when we assign a name we bring something into being and delineate its boundary. We argue about the meaning of names, such as ‘Middle Palaeolithic’ or ‘Acheulean’. However, because traditionally many digital products of our analytical workflow are rarely made public, we don’t put as much care and effort into naming them. Many archaeologists have the mindset that files created as part of the data analysis of a project are their private property for immediate use, and so little thought is given to naming them in a way that might save time and make it easier for these files to be resued. In large projects this may be less true, but in my experience even when working with a dozen or more collaborators, file naming is often chaotic. Below is a sample of poorly choosen file names: myAbstract.docx Ben’s Filenames Use Spaces and Punctuation.xlsx figure 1.png Fig 2.png paper FINAL v.3 Jan16 pleasefixyoursection.pdf To help with this chaos, and make files easier to use and reuse we can use are two principles for naming files: make the file names machine readable; and make them human readable. Here is a sample of more effective filenames that these guidelines help us to make: 2016-09-17_abstract-for-saa.docx bens-filenames-are-getting-better.xlsx fig01_scatterplot-lithics-chert.png fig02_histogram-lithic-chert-flakes.png There are three componenets of machine readable file names that are particularly relevant in research contexts. The first is that file names do not contain spaces, punctuation marks (with some exceptions) or exotic characters such as accents. Machine readability is also improved by consistent capitalisation of letters, since to a computer upper and lower case instances of the same letter are almost as different as A and Z are to us. The second component of making file names machine readable is to make them easy to compute on. File names that are easy to compute on make deliberate use of delimiters such as the hyphen and underscore. In the examples above, hyphens are used to delimit words in the file name, and numbers in the data. But the underscore is used to delimit units of metadata in the filename, for example to separate the date from the description, or the figure number from its description. This careful use of delimiters means that it is possible to extract metadata from the file names. For example, we can use R to compute on the file names to get data relevant to our analysis: # read in the file names my_files &lt;- list.files(&quot;data/file_naming/&quot;) # Make a data frame of the units of infomation in the file names library(stringr) library(dplyr) str_split(my_files, pattern = &quot;_&quot;, simplify = TRUE) %&gt;% as_data_frame() ## # A tibble: 6 x 4 ## V1 V2 V3 V4 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2017-03-03 Area-3 Unit-1 lithics.xlsx ## 2 2017-03-03 Area-3 Unit-2 lithics.xlsx ## 3 2017-03-03 Area-3 Unit-3 lithics.xlsx ## 4 2017-03-03 Area-3 Unit-4 lithics.xlsx ## 5 2017-03-05 Area-4 Unit-1 lithics.xlsx ## 6 2017-03-05 Area-4 Unit-2 lithics.xlsx Thoughtful use of delimiters can make working with a large set of files very efficient. This is because we can use R to search for specific files or groups of files and we can extract information from the file names, as demonstrated above. The third componenet of machine readability is ensuring that file names start with something that works well with default ordering. Often the simplest way to do this is put something numeric first, such as a date, or other sequence of numbers. Then we can easly use the desktop view on our computer to sort our files according to the date or other number at the start of the file name. There are a two more requirements to follow when making file names play well with ordering. You must use the ISO 8601 standard for dates (YYYY-MM-DD) otherwise the dates will not automatically be ordered in the way you expect. If you’re not using dates but a simple numeric sequence, you must left-pad the numbers with zeros. This is important to ensure that, for example, values 10 through 19 are recognised by your computer as greater than 2. A good example of numeric prefixes to filenames is 001_, 002_, 003_, etc. A bad example is 1_, 2_, 3_, because 10_ will be sorted by you computer and placed between 1_ and 2_, rather than after 9_, as you would be expecting. The princple of left-padding file names will also work when prefixed by characters, for example, Fig-01_, Fig-02_, Fig-03_ will still work well with default ordering. Making file names readable for humans mean including information about the contents of the file in its name. When a file name contains some information about its contents, it is easy for other users to figure out what the file is, and make a decision about how to use it. 4.3 Spreadsheets organisation Spreadsheets are ubiquitous in archaeology, as they are in many other domains. For many researchers a spreadsheet is the hub of their data anlaysis activities: they collect data into a spreadsheet, they analyse and visualise the data directly in a spreadsheet, and they copy and paste from the spreadsheet into their reports and manuscripts for publication. Spreadsheets are good for collecting data, but they are inefficient for data anlysis (when you want to change a variable or run an analysis with a new dataset, you usually have to redo everything by hand), and they make your analytical workflow difficult to track. This is because they primarily work by mouse-driven drag-and-drop operations and so spreadsheets do not impose an easy to follow linear order to your analysis. In short, using spreadsheets for data analysis is bad for the reproducibility and transparency of your research. In this section I will review some rules for organising data in spreadsheets that make the data easier to use downstream in your analysis workflow. I am imagining that you will be using R to analyse and visualise the data in the spreadsheet, but these rules will also be good for any programming language, such as Python or Julia. The overall principle that unites these rules is ‘tidy data’ (Wickham and others 2014), which has three characteristics: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Make data ‘tidy’ means getting datasets organised in a way that makes data analysis possible, or better, easy. The rules relating to the principle of tidy data are simply, first, put all your variables in their own columns - where a variable is the thing you’re measuring, like ‘weight’ or ‘length’, and second, put each observation in its own row. The most common violation of these rules is a failure to keep each variable in its own column, with one column representing more than one variable. The table below is two thirds tidy, with each observation (i.e. each archaeological site) as a row, and each observational unit as a table, but is a typical example of a violation of having each variable in its own column. In the Site_type column we see h (for hearth), as h (artefact scatter and hearth), as kf (artefact scatter and knapping floor), and so on. In this form, it is difficult to subset the sites, for example to extract only the sites containing hearths. A more tidy approach would be to have one column for hearth, one column for artefact scatter, and so on. Then each of these columns can be operated on individually, and we have more fine-grained control over the data. untidy_data &lt;- readxl::read_excel(&quot;data/untidy_data.xlsx&quot;) knitr::kable(untidy_data) Suvey_area Site_type Site_size A h 5 A as h 9 A as kf 30 B as kf 12 B as 40 B as kf h 32 Combining multiple pieces of information into one cell is often tempting beacause it makes data collection faster because less time is spent navigating between fields. If you find it impossible to avoid combining information into one cell, or the data were collected by someone else and has this problem already, there are methods in R for cleaning and separating that we will explore in Chapter X. The main point here is to clean and tidy the data before you analyse it, and before you archive it for others to use. The second characteristic of tidy data, each observation forming a row, is easy to accomplish when recording data from a set of speciments, such as artefacts or faunal remains. In that situation it is natural for one artefact to be represented by one row of the table. It is less natural to follow this rule when, for example each row is a site, and some of the columns are counts of pottery types. The proper tidy form in this case would be one row for each site-pottery type combination, but this is a very inconvienent form for data collection. So during data collection we can tolerate a little untidyness for the sake of optimising the speed and convienence of data entry. In Chapter X we will explore methods for making field-collected data truely tidy and suitable for analysis and visualisation. By contrast, the third characteristic of tidy data, that each type of observational unit forms a table, is simple to implement. In the context of working in a spreadsheet, the implication is that each sheet contains only one table. Each sheet should have a single contiguous rectangle of data. This means resisting the temptation to arrange a grid of small tables across a single sheet. Although this a common strategy for organising data because it makes it easy to glance across the small tables to compare them, it is problematic because each row in that sheet contains more than one observation because one row spans several small tables. Similarly, column names are likely to be duplicated because they appear in each of the small tables. These complications make it harder to clean and tidy your data into a form that is useful for analysis and visualisation. The rule is one table per sheet, and if you follow that rule you will make the final tidying of your data for analysis much simpler and quicker. Making small changes to the way you organise your data in spreadsheets, can have a great impact on efficiency and reliability when it comes to data cleaning and analysis. knitr::include_graphics(&quot;images/messy_ktc_data.png&quot;) Figure 4.1: This screenshot shows archaeological data that is untidy for several reasons, including multiple tables on one sheet, multiple values in one column (e.g. SW1), and multiple header rows in a table 4.4 Cell organisation At the most granular level of data collection is the individual spreadsheet cell or data entry field. Bad habits in data entry can require substantial time to fix before those data are suitable for analysis, and lead to errors in the results, regardless of what software yo use. Here I briefly review some of the common errors of cell organiation in the hope that if you are aware of them you might be motivated to avoid them in your data collection. Choosing a good null value requires careful thought. Where you know that the value is zero, of course you should enter zero. But where the value was not measured, or could not be measured, you don’t want to enter zero because that implies that you know something about the value. You also don’t want to leave that field blank, because that could be interpreted as having missed to record that data. Blank cells are ambiguous - did the data collector skip that field by accident, or is were they not able to record a value for that field, or was the value zero? The best choice for the value we need in this situation is NA or NULL (White et al. 2013). NA is a good choice because it is a special (or reserved) word in the R language, so R can handle NAs easily. NULL might be better if your data uses NA as an abbreviation, such as “North America” (NULL is also a special word in R) Another common pain point when moving from data collection to data analysis is where spreadsheets using formatting to convey information. For example, when a cell has coloured text or highlighting to indicate some extra information about that specimen or record. Related problems occur when bold or italic text is used to signal some important quality about the data. These types of are well-established in tables in publication, for example using bold to indicate values that are statistically significant. But at the point of data collection that use of cell and text decoration to convey information makes it very difficult to calculate on those values. It is always better to have another column to capture that information, rather than highlighting the cell or formatting the text. knitr::include_graphics(&quot;images/messy_ktc_data_cell_decoration.png&quot;) Figure 4.2: This screenshot shows spreadsheet formatting that is usesd to convey information. In this case, this yellow highlighing indicates that complete ceramic vessels were found in that level. A better approach would be to have a column called ‘complete_pots’, with values indicating the number of complete vessels in each unit Related contaminants of spreadshet data include merged cells, cell comments and the inclusion of measurement units in a cell, such as the percent symbol. Merged cells can make it difficult to sort and filter data, and difficult move your into other programs for analysis. The proper place for comments is a ‘comments’ column, and units should be expressed in the column header, with only the same measurement unit used for every row in a column. When adding headers to your columns, take care not to include spaces, numbers, or special characters. A good alternative to spaces in column headings are underscore (&quot;_“), for example,”Max_len_mm&quot; or “MaxLenMM” is better than “Max len”, for a column of maximum length values in milimeters. Short or abbreviated field names (i.e. column headers) are often desirable during data collection to save screen space, but they may become confusing later when you revisit the data for analysis. So take care when devising column headers so they are not too ambiguous, and consider keeping a code book or data dictionary that pairs each column header with your description of the variable and how it was measured. 4.5 Summary In this chapter we have reviewed principles, guidelines and rules for collecting data to maximise the ease of analysing that data in R. We considered some general principles of file organisation and file naming to keep your projects orderly and easy for you to keep track of your work over the duration of your project, as well as for others to inspect and resuse your work in the future. We explored methods to enhance the machine- and human-readability of your files. We introcuded the concept of ‘tidy data’ and some practical applications of this approach to spreadsheets. We noted were data are often untidy because of how they are collected, but proposed some rules to ensure data are as tidy as possible at the moment of collection. Finally we looked at the individual cell or field to identify some bad habits that are often seen in other people’s data. We proposed some simple rules for data entry into spreadsheet cells (i.e. sensible null values, not using formatting as data, not merging cells, etc.). The overarching concept is to excersice restraint when working with spreadsheets, and resist the temptation to engage all of its features. Be mindful of the downstream use of your data, by you in R, and by others in other softare environments. We can’t optimise our data collection for every possible data anlaysis situation, but with a few rules we can improve the organisation of our data to make it a lot easier to analyse it with other toolds. References "],
["gettingdatain.html", "Chapter 5 Getting data into R 5.1 Overview 5.2 Getting data from spreadsheets 5.3 Tabular data from non-spreadsheet programs: PDFs and Microsoft Word documents 5.4 Getting tabular data out of unstructured files 5.5 Summary", " Chapter 5 Getting data into R 5.1 Overview In this chapter we will survey some of the key methods for getting data into R. The canonical method for many years has been to use the read.csv function with a CSV (comma separated variables) file. This is a decent approach because most file types that contain tabular data can be saved as CSV, for example, we can save Excel files as CSV using Microsoft Excel. CSV is also ideal because it is an open, plain-text format that can be widely read and edited by many different programs. It is suitable for long-term archiving because it is a stable format. However, it is often more convienent to work directly with the file that the data were entered into, and convienent to work with a format that our collaborators are comfortable with, such as an Excel spreadsheet. Fortunately, there are several packages that allow Excel, other common tabular data file formats, and even databases, to be read directly into R. The focus of much of this chapter is getting data stored in spreadsheets into R, ready to analyse. If you have spreadsheet-like data files from other software, such as SPSS, you can use read_* functions from Hadley Wickham’s haven package in much the same way that you see read_excel used in this chapter. R is also capable of handling more challenging situations, such as working with hundreds or thousands of data files, reading tabular data from Microsoft Word documents or PDFs, and reading in data from unstructured plain text files. In this chapter we will briefly demonstrate how to handle these situations. 5.2 Getting data from spreadsheets 5.2.1 One spreadsheet file, one or more sheets Although the CSV file might be the ideal format for tabular data, and the simplest to work with in R, in our day-to-day work with diverse collaborators usually means that we need to get data in and out of Microsoft Excel. There are many packages for working with Excel files (e.g. gdata and xlsx), but the one I recommend is readxl by Hadley Wickham because it is the simplest to install and use. You can install it with install.packages(&quot;readxl&quot;). Here is a demonstration of a simple use of this package to read in one sheet of one Excel file: library(readxl) jerimalai_sq_a &lt;- read_excel(&quot;data/jerimalai_lithics.xlsx&quot;) # if you have a CSV file, use read_csv() from the readr package As you type that code to import your own spreadsheets, you can save some typing with a special tab-complete function in RStudio. After you type read_excel(&quot;&quot;), move your cursor between the two double quotation marks, and press tab (or Control+Space). A small window will pop up at that location, giving you a list of items to choose from, saving you from typing the full names of the path and file. Figure 5.1 shows an example of this, where you can see that I have tabbed once to get a list and select the ‘data’ folder, and then I’ve tabbed again to get a list of the contents of that folder. Then I can use the arrow keys to select, or type the first few letters of the file I want to complete the command. This kind of tab-completion is possible for many R functions, and can substantially increase the efficiency of your coding by reducing the amount of typing you need to do, and saving you from making spelling mistakes. Figure 5.1: Schematic of file organisation for a simple example of using an Excel file with R. Before discussing the new details in the example above, let’s briefly review the elements of this code that will be familiar from Chapter 1. The first line of the above code chunk, library(readxl), loads the functions in the readxl package into our current R session. The second line contains a function, read_excel(), this does the work of extracting data from our Excel file. We also see the assignment operator, &lt;-, which takes the output from the read_excel() function and sends it to be stored in a new object that we have decided to call jerimalai_sq_a. This new object is a data frame, which is similar to how our data looks when we browse it in Excel. The data frame is a special kind of object in R for tabular data. Columns can be of any class of data (e.g. numeric, character, logical, etc.), but every value in a column must have the same class (i.e. we cannot mix numerics and characters in a single column, the numerics will be converted to characters). We can see the column names are as we expect them to be. If you want to get directly to working with your data now that you’ve imported it, skip to Chapter XX, otherwise read on to learn more about importing data. In this example we have an Excel file called jerimalai_lithics.xlsx that is located in a folder called data. Recalling the advice in Chapter 1, the first step when starting a session of work on data analysis is to set our Working Directory to the ‘Source file location’ (go to RStudio toolbar menu, click on ‘Session’, then ‘Set Working Directory’ then ‘To Source File Location’). This means that if we’ve followed the file organisation recommendations of Chapter 2, we have a folder called data in the folder that contains the source document (i.e. this document that you are reading), then we can easily connect to files in that directly. In Figure 5.2 we see a schematic of this file organisation, with the source document to the left, and the Excel file in the ‘data’ folder to the right. Figure 5.2: Schematic of file organisation for a simple example of using an Excel file with R. The Excel file in this example contains some of the data collected from excavations at Jerimalai rockshelter, East Timor (Marwick et al. 2016). One row represents one stone artefact, and there are two sheets, one for each excavation square. To access the other sheet, we can look at the documentation for read_excel() by typing ?read_excel in our console and pressing Enter. We can see in the ‘Usage’ and ‘Arguments’ sections of the documentation (Figure 5.3) that there is an argument called sheet which we can use to specify what sheet in the Excel will be read into R. The text in ‘Arguments’ tells us that we can refer to sheets by number or the actual name of the sheet as it is in the Excel file. In the ‘Usage’ section we can see that the default value for sheet is 1. This means that if we don’t specify what sheet we want, we will automatically get the first sheet in the Excel file. Figure 5.3: Screenshot of the help page for the function read_excel. We can skip over most of the details on the read_excel() help page, but there is one more argument that is worth nothing because it is very handy. The skip argument allows us to specify a number of row to skip when importing from and Excel file. This is especially useful when our speadsheet has a row with merged cells at the top, perhaps to indicate groups of similiar variables (i.e. columns). We usually want to skip over these rows with merged cells when we import data from spreadsheets like this. For example, we might do read_excel(&quot;data/jerimalai_lithics.xlsx&quot;, skip = 1) to skip the first row of our spreadsheet (our example spreadsheet does not contain any merged cells in the first row, so we don’t need to skip it for this spreadsheet). If we wanted to read in the second sheet of our Excel file, which contains data on the stone artefacts from excavation square B, we have two options: jerimalai_sq_b &lt;- read_excel(&quot;data/jerimalai_lithics.xlsx&quot;, sheet = 2) # using the sheet number # or we can do it like this: jerimalai_sq_b &lt;- read_excel(&quot;data/jerimalai_lithics.xlsx&quot;, sheet = &quot;Jerimalai_All_Artefacts_Sq_B&quot;) # using the name of the sheet in our Excel file I prefer to use the sheet name for the sheet = argument because if the order of sheets in the Excel file changes the function will no longer read in the correct data when sheet = 2. Now we have read in two sheets from our Excel file, and we have two data frames ready to work with in our R session. But what if we have a large number of sheets in a single Excel file? In this case it will be tedious to repeat these lines many times, changing the sheet name for each copy of the lines. Avoiding repititive tedium is one of the strengths of using a programmming language for data analysis, and here is how we can solve this particular problem: # create a character vector of sheet names jerimalai_sheet_names &lt;- excel_sheets(&quot;data/jerimalai_lithics.xlsx&quot;) # iterate over the vector of sheet names to read in the data from each sheet library(purrr) jerimalai_all_sheets &lt;- map(jerimalai_sheet_names, ~read_excel(&quot;data/jerimalai_lithics.xlsx&quot;, sheet = .x)) In plain English, the above example uses the excel_sheets() function (from the readxl package) to get the sheet names from our Excel file. We have just two sheets in our example, but this same approach could be used for 10, or 100, or more sheets. The object jerimalai_sheet_names is a character vector containing the names of the sheets in our Excel file. The next part of the example loads functions from Hadley Wickham’s purrr package, and then shows how we can use the map() function to apply to the read_excel() function to each sheet of our Excel file. For each item in the jerimalai_sheet_names vector, that is, each sheet name, the map() function will apply the read_excel() function, and store the resulting data frame in jerimalai_all_sheets. There are two important details in the map() function to note here, first is the ~, which we use to indicate that we want the function read_excel() to operate on each element of jerimalai_sheet_names, and second is the .x which is a place-holder for each specific element of jerimalai_sheet_names. In the above example we take advantage of a very powerful and efficient programming concept known as a loop or for-loop. This is an instruction to the computer to repeat a piece of code a specific number of times. In our example, the computer will repeat read_excel() two times, once for each sheet in our Excel file. During the first repitition, the .x in the map() function takes the value of Jerimalai_All_Artefacts_Sq_A, the first sheet name, and during the second repitition the .x becomes Jerimalai_All_Artefacts_Sq_B, the second sheet name. With just two repititions, this complexity might seem like overkill, but you can imagine how much time we would save if we had 50 or 500 sheets. There are other methods for writing for loops in R (e.g. using for() and lapply()), but I prefer map() for most occasions because it is quick to type and easy to read and understand when I come back to my code after some time away and can’t immediately remember what is going on. For further reading about other methods of writing loops in R, I recommend De Vries and Meys (2015), Matloff (2011), or Wickham (2014). The result of our map() function is a list of two data frames. A list is a very useful data type in R, and worth to get comfortable with. So far we have used vectors (a single sequence of data elements), such as the character vector jerimalai_sheet_names that holds the sheet names of our Excel file, and data frames, which are tables of data similar to what we see in Excel spreadsheets and other tabular structures. Often our data frames have one observation per row, with each column representing a variable, a measure, feature, or characteristic of that observation, but more exotic uses are also possible, such as storing lists in rows. Our jerimalai data frame is a good example of a typical data frame, with each row representing a single stone artefact (or observation) and each column represented a variable that we recorded for each artefact. So how is a list different from a vector or data frame? A list is a generic vector that can contain a mixture of any kind of object. We can have a list or several numeric vectors, or a list that includes several numeric, character and logical vectors, or a list of data frames, or a list of lists, and so on. Lists are the most versatile objects in R because we can store anything in them. 5.2.2 Mulitiple spreadsheet files If we have a handful, or hundreds of spreadsheet files, we can take advantage of the same looping concept that we used for reading multiple sheets to quickly read all these files into our R session. In the example below we have a folder called many_excel_files that contains two Excel files (but it could be any number, 10, 100, 1000 or more files). We will make a character vector of the Excel file names, then we will loop over each file name to extract the data from each Excel file, one at a time, and collect the results in a list: # get the file names and store in a character vector my_numerous_excel_files_names &lt;- list.files(&quot;data/many_excel_files&quot;, full.names = TRUE) # apply the read_excel() function to each file in our vector of file names my_numerous_excel_files_data &lt;- map(my_numerous_excel_files_names, ~read_excel(.x)) In this example we see the function list.files(), which is very useful, and comes built-in with R (we don’t need to install a package for this one). As we use it here, list.files() creates a list of all the files in the many_excel_files folder. But sometimes we don’t want to read in all the files, just a set of them, according to something they have in common. We can use the pattern argument to restrict the files that list.files() will list. For example, if we have a mix of text files and Excel files in a folder, but we only want to get a list of the Excel files, we can use pattern like this: only_my_excel_files &lt;- list.files(&quot;data/many_excel_files&quot;, pattern = &quot;.xlsx$&quot;, full.names = TRUE, ignore.case = TRUE) When we use pattern = &quot;.xlsx$&quot;, we are telling the computer that we only want to list files that have ‘.xlsx’ at the end of their filename. The dollar sign requires that the characters .xlxs match our search only when they are at the end of the filename. For example, this filename would not match this pattern: names_of_.xlsx_files.docx because although it contains the string .xlxs, the end of the filename is docx, so it doesn’t match our pattern. We can capture files ending with ‘.xlsx’ or ‘.XLSX’ by adding ignore.case = TRUE, which will ignore differences between upper and lower case characters. We can also use pattern to match other parts of the file names using regular expressions. For example, if we have many spreadsheets that are named in this format: ‘region_name-site_name-square_name-recorder_initials.xlsx’, we can write R code to select files that have the characters ‘BM’ only after the third ‘-’ to get all the files for regions/sites/squares that I recorded. We will discuss how to use regular expressions to do this kind of selection in Chapter XX. Our use of map() in the above example is very similar to how we used it when we read in multiple sheets from one Excel file. We can see the common elements ~ (indicating the function to be repeated) and .x (a place-holder for each element of the vector that the function will operate on) in both examples. Similarly, the output we receive from the map() function in this example is a list of data frames. This approach is especially useful when we have a large number of Excel files that are similarly structured (i.e. the same number of columns with the same column names) because then we can also use map() to analyse the visualise that data. 5.2.3 Importing messy spreadsheets, or when is it ok to manually edit the data? In the examples so far, we have assumed that the data in the spreadsheet is a tidy rectangle in the upper left corner of the sheet, and does not contain any formatting or formulas that are important for our analyses. We introduced skip as a method to ignore rows with merged cells or other information that we don’t need. However, what can we do if our Excel file contains a set of tables in a single sheet? Or when some bold text in some cells conveys important important information? The rexcel package by Rich FitzJohn and Jenny Bryan and the tidyxl package by Duncan Garmonsway can help with capturing data, formulae and formatting information from an Excel sheet. The jailbreakr package, also by Rich FitzJohn and Jenny Bryan can help with Excel files that contains a set of tables laid out on a single sheet. Similarly, the unpivotr package by Duncan Garmonsway can help with importing Excel spreadsheets containing complex or irregular layouts of into regular data frames in R. Although these methods can make working with messy spreadsheets less painful, they raise the question of whether we should simply edit the spreadsheet manually so that it’s more convienent for importing into R (e.g. moving the each of the tables onto their own sheet or own file, or creating a new column to represent the variable that was indicated by the bold text formatting). Manually editing is tempting because it could be quicker and less frustrating than importing the file as-is and write R code to arrange the data into a more useful form. The decision about whether to manually edit the data, or write code to arrange the data can be difficult. In my experience, it can take up to an hour or two to work out the code to take data from a messy spreadsheet and arrange it into a convienent form in R. On the other hand, it can take just a few minutes to manually edit the spreadsheet to prepare it for importing into R and be ready for use. If time is the only consideration, the choice may be obvious. However, manually editing the data is problematic because it can involve decisions that change the results of your analysis, but leave no trace and are impossible, or very inconvienent, to reverse. Making undocumented decisions in the data analysis workflow violates a key principle of reproducible research, that every decision about the data analysis should be documented in the analysis scripts. One way around this is to write a brief note that describes how you altered the data from its original form to the form you used to import into R. This note is included with the original, unaltered data, your new modified data in your project compendium so anyone can trace your steps in editing the data. There are some software packages that can help with this (e.g. https://datproject.org/), but I have not seen any in regular use among social scientists, so I am reluctant to make specific recommendations. One factor that might help you decide where to manually edit a spreadsheet or to write code to workaround the messy spreadsheet is whether or not other people need to edit the spreadsheet during to time that you are working with it. If your collaborators are still updating and circulating the spreadsheet while you are working on the analysis, then it may be disruptive if you reorganise the sheet in the middle of this process. In this situation it may be preferable to write code to handle the messiness. On the other hand, if the data in the spreadsheet is final and no further changes are being made by other people, then manual editing wont be disruptive to the collaboration process, and might be a better option. 5.3 Tabular data from non-spreadsheet programs: PDFs and Microsoft Word documents While spreadsheets and spreadsheet-like data are easy to import into R, occasionally we need to get tabular data out of other kinds of documents for our analysis. This is often the case when we want to use data published in a report or as supplementary materials to a journal article. For small tables it may be practical to transcribe them into a spreadsheet by hand, and then read that spreadsheet into R using the methods described above. However, when you have large tables, or many tables across many pages or many documents this is undesirable because it is time consuming and will likely introduce transcription errors. Below is an example of how we can extract a table from a specific page of a PDF file of a journal article. Using Thomas Leeper’s tabulizer package, we specify the file to operate on, and the page (or pages) to look for tables on. Consider the table in Figure 5.4, which is from Terry et al. (2000). This would be tedious to transcribe by hand, so we can use the extract_tables function from the tabulizer package to read the PDF and extract this table to use in R. knitr::include_graphics(&quot;images/page_from_Terry_et_ al_2000_[P].png&quot;) Figure 5.4: A table of a data from a PDF file. In the example below, the extract_tables() function returns a list with one item. If a page has multiple tables, or text and tables, this list would have additional items. In the example below, the list item is a character matrix, which is the table that we want to work with. We can coerce the matrix to a data frame with as_data_frame() from the dplyr package, and then the table is nearly ready to work with in R. There are a few additional steps of cleaning to do before we can work with this table, such as moving the first and second rows out of the table and into the column headings, and coercing the measurement columns from character class to numeric class so they are suitable for numerical operations. We will investigate how to do these tasks when we discuss cleaning data in Chapter X. library(tabulizer) # load the libraries library(dplyr) # read in the table from the PDF file table_from_pdf &lt;- extract_tables(&quot;data/Terry_et_al_2000_[P].pdf&quot;, pages = 9) # inspect the output of extract_tables(), here we acces the first item in the list # and coerce to a data frame as_data_frame(table_from_pdf[[1]]) # save as a CSV file easier downstream use and reuse by others write.csv(table_from_pdf[[1]], &quot;data/data_output/table_from_Terry_et_al_2000.csv&quot;) The tabulizer package also has an interactive mode allowing you to click and drag a rectangle over a page to select the area to extract from. For large PDF files you can use the split_pdf() function to separate out the pages with tables to improve the performance of extract_tables(). Of course, after extracting the table from the PDF you should save it as a CSV file to ensure that it can more easily be resued in the future. We can similarly extract table from Microsoft Word documents using the Bob Rudis’ docxtractr packages.The docxtractr package provides tools to count the number of tables in a documents, identify the table structure, and extract tables from Microsoft Word docx documents. The example below shows how we can extract a table of magnetic susceptibility data from the supplementary materials published in from Haslam et al. (2012). Here is how the table looks in Microsoft Word: knitr::include_graphics(&quot;images/Haslam_et_al_2012_[Jwalapuram].png&quot;) Figure 5.5: A table of a data from a Microsoft Word file. And here is the code for extracting this table out of the complete supplementary materials document: library(docxtractr) # load the library # read in the docx file docx_file &lt;- read_docx(&quot;data/Haslam_et_al_2012_[Jwalapuram].docx&quot;) # extract the second table in the docx file into a data frame table_from_docx &lt;- docx_extract_tbl(docx_file, tbl_number = 2) # view the first few rows head(table_from_docx) ## # A tibble: 6 x 6 ## Sample Depth..cm. lf1 hf2 fd.3 fd4 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 SS1 30 1.7129 1.5590 8.98 153.9 ## 2 SS2 40 1.8777 1.6864 10.19 191.3 ## 3 SS3 50 2.0472 1.8417 10.04 205.5 ## 4 SS4 60 2.2143 1.9761 10.76 238.2 ## 5 SS5 70 2.2806 2.0092 11.90 271.3 ## 6 SS6 80 2.0868 1.8658 10.59 220.9 # write to CSV file for easier reuse and achiving library(readr) write_csv(table_from_docx, &quot;data/data_output/mag-sus-data-from-Haslam_et_al_2012.csv&quot;) Notice that we do not get the Greek symbols in the data frame, nor do we get the comments below the table. The docx_extract_tbl() function only extracts the contents of the table, and exotic characters may be dropped. The docxtractr package does not have an interactive method like the tabulizer does for PDF files, but docxtractr has the docx_describe_tbls() function that can detect cells merged across rows or columns. It does not un-merge those cells, but it will help you identify where they are, so you can be more efficient with data cleaning. 5.4 Getting tabular data out of unstructured files The methods above for working with spreadsheets and tables will likely cover the majority of cases for getting data into R, but occassionaly we have to work with data that is not stored in a convienently structured format. For example, some kinds of analytical instruments output data in plain text files that lack a convenient structure or widely used format. In that case, we can read the file in using more generic file reading functions. Consider this plain text file of excavation data with three columns of data, each column separated by a space: knitr::include_graphics(&quot;images/unstructured_data_example.png&quot;) Figure 5.6: A table of a data from a plain text file (viewed in Notepad++) We can use the function read_table2() from Hadley Wickham’s readr package, this can read plain text files and identify columns that are separated by one or more white spaces. A ‘white space’ is one or more spaces, tabs, newlines characters or carriage return characters (i.e. a press of the ‘enter’ or ‘return’ key). Since our text file has space-separated columns, this function will automatically detect these, and place the data into a data frame. The read_table2() function is very similar to the base R read.table() function, which does not require installation of any packages. However, read_table2() has two major advantages which make it worth installing the readr package: first, the default settings are more convienent (with read.table() we also need to specify stringsAsFactors = FALSE), and it automatically detects the column classes and sets them for us. In the example below we see that the column classes are ‘character’, ‘integer’ and ‘double’, which is what we would expect them to be, and so they are immediately useful for calculation and visualisation, saving us a few steps of data cleaning. library(readr) # load library # read in unstructured data from plain text file unstr_data &lt;- read_table2(&quot;data/unstructured_data_simple_example.txt&quot;) # inspect the result head(unstr_data) ## # A tibble: 6 x 3 ## Square Spit Depth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 C3 1 0 ## 2 C3 2 0.02 ## 3 C3 3 0.05 ## 4 C3 4 0.1 ## 5 C3 5 0.16 ## 6 C3 6 0.22 But what if we had a more complicated plain text document, say with series of table separatd by a blank line, and with each table having a short table on the line above the column headers? Consider the image below, which shows total statin data colleced during archaeological excavations at Madjedbebe (Clarkson et al. 2015): knitr::include_graphics(&quot;images/unstructured_data_complex_example.png&quot;) Figure 5.7: A table of a complex data from a plain text file (viewed in Notepad++) One option is to manually edit the plain text file to save it as many separate files, with one table in each file. This is feasible for a small number of tables, but if we have hundreds of tables in a single plain text file we would prefer a more efficient method. We can use the scan() function to read this file into R, and then perform some operations to get it into a useful form: # read in the file unstr_data_complex &lt;- scan(&quot;data/unstructured_data_complex_example.txt&quot;, what = character(), blank.lines.skip = FALSE) # inspect the output head(unstr_data_complex) ## [1] &quot;first&quot; &quot;set&quot; &quot;5013.038383&quot; &quot;4997.840727&quot; &quot;100.739809&quot; ## [6] &quot;SL_B5_N&quot; The result of the scan() function is a single character vector, where each item in the table is one item in the vector. No structure of the table is preserved in this character vector, we must impose this structure back on to the data, based on our knoweldge of the plain text file. The first detail to work on is that each table is separated by a blank line. To ensure that scan() sees these, we se the argument blank.lines.skip = to FALSE. Then, in the character vector resulting from scan(), we see the blanks lines as an empty element, &quot;&quot;, in the vector. We can split this character vector on these empty elements to separate each table into its own object. First we find the locations in the character vector of the empty elements that represent the blank lines. We can use which() to find the index values where the vector is equivalent to (==) the empty element: blank_line_locations &lt;- which(unstr_data_complex == &quot;&quot;) They are at element numbers 75 and 154. We can use these element numbers with split_at(), a function in the aswr package, to split the character vector at the blank line locations, and get a list of several vectors: library(aswr) list_of_tables &lt;- split_at(unstr_data_complex, blank_line_locations) # inspect the output str(list_of_tables) Because we know that the first element of the second and subsequent items in the list represents the empty line, we can delete it. We firstly select only the second and later elements of the list with list_of_tables[2:length(list_of_tables)], and then we map over those elements and using negative indexing in square brackets to drop the first item in each character vector in those elements of the list: library(purrr) list_of_tables[2:length(list_of_tables)] &lt;- map(list_of_tables[2:length(list_of_tables)], ~.x[-1]) Now we can take the table labels (‘first set’, ‘second set’, etc.) from the character vectors and into the list, to name the elements in the list. We map over each character vector in the list, extract the first two items in the vector with .x[1:2], paste them into a single item with paste(.x[1:2], collapse = &quot; &quot;), and then simplify the result (which woult normally be a list using map()) into a character vector by using map_chr: table_labels &lt;- map_chr(list_of_tables, ~paste(.x[1:2], collapse = &quot; &quot;)) # assign these labels to name the list items: names(list_of_tables) &lt;- table_labels Now the table names are preserved in the list, we can delete them from the character vectors using negative indexing to drop the first two elements of each vector: list_of_tables &lt;- map(list_of_tables, ~.x[-c(1:2)]) Now we want to transform each character vector into a table with four columns, as we see them in the plain text file. Because we know there are four columns, we can use the seq() function to extract values from every fourth position in the vector to extract each column. By starting at the first item and extracting every fourth item we get the first column, then starting at the second item and extracting every fourth item we get the second column, and so on. Then we have four vectors that can be combined into a data frame. I have spelled this out in detail for the first table in the file below: # Just transform the first character vector into a table with four cols: x &lt;- list_of_tables[[1]] col1 &lt;- x[seq(1, length(x), 4)] col2 &lt;- x[seq(2, length(x), 4)] col3 &lt;- x[seq(3, length(x), 4)] col4 &lt;- x[seq(4, length(x), 4)] the_first_table &lt;- data_frame(col1, col2, col3, col4) But we want an approach that will operate automatically on each item in the list, to avoid copying and pasting this code, which is time consumring and error-inducing. The aswr package contains the function df_from_vector() that automates this process for a list of any length, and for tables of any number of columns. We supply the name of the list of character vectors, and the number of columns that each table should have, and we get a list of data frames in return: # process the list of character vectors into a list of data frames list_of_data_frames &lt;- map(list_of_tables, ~df_from_vector(.x, 4)) # inspect the output str(list_of_data_frames) And now that list of data frames presents the data to use in a convienent tabular structure that is amenable for cleaning, analysis and visualisation. Some further cleaning is necessary, such as adding informative column names, but we have saved a lot of tedious manipulation of the file by hand with these few lines of code. However, this example does contain one detail that we can explore further, to better approximate an unstructured data file - that the table names in the raw data data will be exactly the same length (i.e. “first set”, “second set”, etc.). A more challenging data set would have table names of unpredictable length, requiring a slightly different strategy to read in the file and impose a tabular structure onto the text. To tackle this problem, we can start with readLines(), which reads in each row of the raw data as one element of a character vector: more_complex &lt;- readLines(&quot;data/unstructured_data_more_complex_example.txt&quot;) str(more_complex) ## chr [1:61] &quot;first table&quot; &quot;5013.038383 4997.840727 100.739809 SL_B5_N&quot; ... We can follow the same pattern as above by splitting this one large character vector by the empty elements, here we next which() in split_at() for more compact expression, and we can go ahead and remove the empty element (that we used to split on) from the second and subsequent list elements with map(): list_of_complex_tables &lt;- split_at(more_complex, which(more_complex == &quot;&quot;)) list_of_complex_tables[2:length(list_of_complex_tables)] &lt;- map(list_of_complex_tables[2:length(list_of_complex_tables)], ~.x[-1]) Now we have a list, where each element is a character vector, and each element in the vectors is a row of the table. For example, each element looks something like this “5012.884297 4997.24741 100.707435 PF_B6_1_C14X232”, with the four columns combined into one element, separated by a space. Our task now is to extract the table names and remove them from the character vectors, then transform the character vectors into a data frame. Here we assign the first element of each vector to be the names of the list items: names(list_of_complex_tables) &lt;- map(list_of_complex_tables, ~.x[1]) And then remove that first item from the character vector, now that we have preserved it elsewhere: list_of_complex_tables &lt;- map(list_of_complex_tables, ~.x[-1]) Now we have a list of character vectors, and we can separate each item in the vector into four columns. Let’s show how this can be done on the first item in the list, then generalise to all items in the list: library(dplyr) # load libraries library(tidyr) # extract the first item in the list x &lt;- list_of_complex_tables[[1]] output &lt;- x %&gt;% data_frame(y = .) %&gt;% separate(col = y, into = c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;, &quot;col4&quot;), sep = &quot; &quot;, convert = TRUE) In the above example, we first extract the first item from the list, which we store as an object called ‘x’. Then we convert this character vector into a data frame with one column usingdata_frame(), in this function we create a column called ‘y’, by assigning the character vector ‘y’ to this column. In this sequence of operations we are using the ‘pipe’ symbol, %&gt;%, which we can translate as ‘then’, in the example above we can translate the first few lines as “Take the character vector ‘x’, then (%&gt;%), place that vector into a column in a new data frame (data_frame(y = .)), and then separate”, and so on. The specific task that the pipe does is to take the object to its left (in this case x) and pass it to the first function on its right (in this case data_frame(). We can specifically refer to the object taken by the pipe with a period ., so that the period in data_frame(y = .) refers to the object ‘x’ on the previous line. This pattern of piping objects from one function to another is highly readable, and often results in efficient, tidy code that is easy to understand and re-use. After creating the data frame from the character vector, we then (%&gt;%) pass this data frame to the separate() function from Hadley Wickham’s tidyr package. The separate() function is designed to split one column in a data frame into multiple columns, so it is ideal for our problem of splitting the column we have into four new columns. In the separate() function, the first argument is the data frame with the column to split, in our example we this is passed in invisibly by the pipe symbol, and we do not need to type it. So the first argument that we need to explicity supply to separate() is the name of the column in our data frame to separate, which is ‘y’. After that, we need to give the names of the new columns that we want to split ‘y’ into. Here I have simply called them col1 through col4, but they can be almost anything. The next required detail is the character to split ‘y’ on. In this case, we can split on the single blank space between each item in the character column ‘y’. Finally, we set convert = TRUE, this tells the separate() fucntion to automatically detect the column class, and convert the new column to the detected class. The result of this sequence of operations is shown below. We can see that we have a data frame with four columns, as expected, and the columns have sensible classes and are ready for analysis: head(output) To generalise this column-splitting approach for automated repeated use on multiple items in a list we can combine it with the map() function. The only change we need to make to the sequence of operations we described above is to to change ‘x’ to ‘.x’, the rest remains the same. This change from ‘x’ to ‘.x’ means that the input to the data_frame()-separate() sequence changes from the character vector ‘x’ that we extracted above, to the iterator, ‘.x’, which stands in place of each item in the list that we supply to the map() function. list_of_complex_tables_separated &lt;- map(list_of_complex_tables, ~.x %&gt;% data_frame(y = .) %&gt;% separate(col = y, into = c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;, &quot;col4&quot;), sep = &quot; &quot;, convert = TRUE)) In the example above, we start with the map() function because we want to repeatedly apply a set of operations to each item in a list. Our list is the list of character vectors, called ‘list_of_complex_tables’, resulting from reading in the unstructured plain te file, and the first few steps of extracting the table lables. The tilde symbol ~ indicates that what follows that symbol is a function to apply to each element of the list ‘list_of_complex_tables’. For this function, we have pasted in the code that we used to experiment on the first item in the list, and now the map() function will automatically apply this code to each item in the list. All we have to do it wait for it to finish. The structure of the output of this operation is shown below. We have a list with three items, each item is a data frame with four columns, just as expected. str(list_of_complex_tables_separated) You may never encounter unstructered data exactly like these examples, but this general pattern is likely to save time and frustration when working with data in unsual formats. The general pattern that the above examples use is: Read in the plain text file using a generic function such as read_table2(), scan(), or readLines(). If the unstructured data is in a Microsoft Word document, you may need to open that document in a word processer and save it as a plain text file (i.e. .txt) to read it into R. If the data are in a PDF, Jeroen Ooms’ package pdftools can read the PDF into R as a character vector. Inspect the resulting object with str() and head() to evaluate how best to proceed. If there are multiple tables in your file, split the object into the individual tables as items in a list object using split() or the more convenient derivative function split_at(). Extract table labels or empty elements so that the only items in the list are the data values that you want to see in a data frame. We used map() and sqaure braket indexing (e.g. [-1] to remove the first item in a vector) to do this in the above examples. Separate or split the character vector in each list item into columns in a data frame. The above examples demonstrate two of methods for this task, including the convienent functions df_from_vector() and separate(). We demonstrated how it is good practise to experiment on the first item in the list to explore how to get the desired output. Then we adapted the code for this experiment to apply it repeatedly to a list of items by using the map() function. Finally, inspect the output with with str() and head() to check that the format is as you expect, and save is at one or more CSV files to make it easier to reuse these data. 5.5 Summary In this chapter we have surveyed several methods for getting data into R from data ‘containers’ that are commonly used in archaeology. We have described how to get data from one spreadsheet file, one file with many sheets of data, and many spreadsheet files. We have also looked at more challenging situations where a table of data is in a Microsoft Word document or a PDF. With the aid of some contributed packages, R can import tabular data from these files with relative ease. Finally, we explored several methods for handling unstructured, or less structured data. These methods are more complex and will vary according to the specific characteristics of the data you are attempting to import. We concluded these more complex data import examples with a set of general steps to guide you when importing more challenging data sets. It is impotant that you save the scripts you write to get data into R, or if the import is simple, include the read_*() lines in your analysis code. Similarly, keep the raw data files stored close to your R scripts, using the file organisation suggested in the previous chapter. This will make it easier for you to retrace your steps, if you need to diagnose an error or check an unexpected result. It will also make it easier for others to reproduce your results to determin the reliability of your work. Where it is not possible to store your data locally, or share it with your publication, you need to explain in detail the origin of your data so that your readers have enough information to make a decision about the reliability of your data. Even if you cannot share your data, you should still share your data import scripts because this may contain information that will help your readers understand how your data had been modified during your project. In the following chapter we continue the process of getting data ready to analysi by demonstrating common methods for cleaning and tidying data. This next chapter will help to close the gap between getting the data into R and doing useful analysis with the data. References "],
["preparingthedata.html", "Chapter 6 Preparing the data for analysis 6.1 Overview 6.2 Navigating data 6.3 Five concepts for cleaning data 6.4 Wide and long data 6.5 Dealing with missing data 6.6 Joining data together", " Chapter 6 Preparing the data for analysis 6.1 Overview It is rarely the case that data that are fresh from field work or obtained directly from an instrument are immediately ready for analysis and visualisation. Often there are minor inconistences and errors in the data that need to be cleared out of the data. Identifying and removing these contaminants is rarely taught during undergraduate or gradute training, and as a result, many researchers spend a lot of time struggling with making their data fit for analysis and visualisation. The aim of this chapter is to investigate five key concepts for cleaning data with R. This chapter will give you the tools to identify problems with your data, fix them quickly and simply, and get your data into a suitable shape for analysis and visualisation. First we will ensure that you can navigate your data easily, then we will describe five data cleaning concepts, and finally we will explore some methods for joining tables together. 6.2 Navigating data The most natural container for tabular data in R is the data frame. Data frames have serveral variants with slightly different properties. For example in this book we frequently use the tibble class, a trimmed down version of a data frame, because it provides neater and more informative output when we print a tibble to the R console. But the core data frame functions remain the same. For example we can extract the first row of a tibble data frame called ‘my_df’ using the square braket subsetting method like so my_df[1, ], and extract the third column like this my_df[ , 3]. On the left side of the comma in the middle of the square brakets we control how to subset the rows of the data frame, and on the right side we control how to subset the columns of the data frame. This method of square braket subsetting is useful for quick interactive exploration of the data during the data cleaning steps. The examples below show how to use the square brackets for a variety of different subsetting tasks: my_df[1] # first column in the data frame (as a data frame) my_df[, 1] # first column in the data frame (as a vector) my_df[[1]] # also the first column in the data frame (as a vector) my_df[1, 1] # first element in the first column of the data frame (as a vector) my_df[1, 6] # first element in the 6th column (as a vector) my_df[1:3, 7] # first three elements in the 7th column (as a vector) my_df[3, ] # the 3rd element for all columns (as a data frame) my_df[&quot;col1&quot;] # Result is a data.frame my_df[, &quot;col1&quot;] # Result is a vector my_df[[&quot;col1&quot;]] # Result is a vector my_df$col1 # Result is a vector Because it can be confusing to predict the output class when using square brackets, we will use a different and more consistent method for subsetting during data analysis. But these methods are good for quickly inspecting your data and identifying cleaning tasks. In some cases a matrix may be a better container for your data than a data frame, or your data may be transformed into a matrix by a function you’re using. Matrices have similiar subsetting methods to data frames, so for the purpose of this chapter we will group them together. You may want to choose a matrix over a data frame for your data if your rectangular data is all one class (i.e. all character or all numeric), is relatively big (i.e. hundred to thousands or more or columns or rows), and you don’t need column names, then a matrix will probably take up less space in the memory of your computer and be faster to analyse. A small number of data frames are can be efficently managed as individual objects in your R environment without confusion, but if you have a dozen or more data frames of similar sizes you will find yourself repeating code unnessearily as you repeat functions on each of the data frames. This can result in mistakes in your code and wasted time. A more efficient way to work with multiple data frames is to store them in a list. We have used lists extensively in the previous chapter as a conivenent way to store multiple data frames and operate on them. The square bracket method of subsetting also works on lists, and you’ll notice that the double square brackets we can use to extract a column from a data frame is used extensively for lists. This is due to the fact that on a technical level a data frame is a list of vectors of equal length (Matloff 2011). Consider a simple list with three items, and the items are names a, b, and c: my_list[[1]] # first item in the list (as the class of that item) my_list[[3]] # third item in the list (as the class of that item) my_list[1] # a list containing only the first item my_list[1:3] # a list containing only the first three items my_list[c(1, 3)] # a list containing only the first and third items my_list[[&#39;a&#39;]] # first item in the list (as the class of that item) my_list[&#39;b&#39;] # a list containing only the second item in the list my_list$c # third item in the list (as the class of that item) The general pattern is that double square brackets including an ‘unlist-ing’ step in extracting an item from the list, while the single square brackets preserve the list structure. For example, if you have a list of data frames, then ‘my_list[[1]]’ will return the first data frame in the list, as a data frame. However, ‘my_list[1]’ will return a list containing the first data frame. These might seem like arcane subtleties, but its worth to take care when subsetting a list to minimize frustration and ensure that the result is a class that is useful to your analysis. 6.3 Five concepts for cleaning data Now that we have our data imported into R, and we have some familiarity with how to access it, we can work on cleaning and tidying the data to make it ready for analysis and visualisation. Data cleaning can profoundly influence the results of a statistical analysis, so it is important to take care with these steps. I have divided this process into five common tasks that I routinely do when cleaning for analysis (cf Jonge and Loo 2013): Fixing names: correting spelling, replacing spaces, etc. Converting column classes: character to numeric, extracting numbers from strings, etc. Splitting and combining columns: separating one col into two or more Reshaping: from wide to long and long to wide 6.3.1 Fixing names Fixing names refers to two tasks: putting the correct column names on a data frame, and correcting misspellings of names of items with columns in the data frame. A common problem with column names on data frames is that the ones your expect are missing, and instead you have column names like X1, X2, X3… or V1, V2, V3… We encountered this problem in the previous chapter when we used the extract_tables() function from the tabulizer package to extract a table from a PDF. The first challenge with the output of that function is that it returns a matrix: class(table_from_pdf[[1]]) So we use as_data_frame() from the dplyr package to convert this into a tibble data frame (there is a as.data.frame() function for which no package is needed, but I find that the dplyr version has more useful defaults settings). We need to convert the matrix into a data frame so that we can have column with different classes, for example the ‘Sample’ column needs to be character class so it can hold a mix of letters and numbers, and the measurement columns need to be numeric so we can compute on them. So we can convert to a data frame, and then use head() to inspect the output: terry_table &lt;- as_data_frame(table_from_pdf[[1]]) head(terry_table) In the output we see that the actual column names are V1, V2, V3, etc., and the column names we want are in the first and second row. If the column names were exclisvely in the first row, we could assign the first row to the data frame column names, and then delete the first row. The pattern for this is: names(my_df) &lt;- my_df[ 1, ] # assign first row to the column names my_df &lt;- my_df[-1, ] # delete the first row Or for the same result in one line, we can use the assign_colnames() function from the docxtractr package. In the example below the :: saves us from having to type library(docxtractr) to make the assign_colnames() function available to our R session: # move the first row of the data frame to the column names my_df &lt;- docxtractr::assign_colnames(my_df, 1) This is an ideal solution for this common problem where the column names are in the first row and we want to move them to the proper place. However, our example here is slighly more complex because the column names are spread across the first and second rows of the data frame. In this case, the general strategy is to create a character vecttor that is the result of pasting together the first and second row for each column, then proceed as above and move the first row to the column names. In the code below we take the first and second rows of the data frame (terry_table[c(1:2), ]), and use the paste() function to combine them into a a single character string for each column. The collapse = &quot; &quot; argument to to the paste() function indicates that we want to collapse the two items into one item where they are separated by a space. So that “Total P” (row 1 col 4), and “(mg/kg)” (row 2 col 4) become one item: “Total P (mg/kg)”. To automate this process across each column of the ‘terry_table’ data frame, we use the map_chr() function, which also converts the output to a character vector (compare to the generic map() which returns a list): # extract and combine first two rows terry_table_col_names &lt;- map_chr(terry_table[c(1:2), ], ~paste(.x, collapse = &quot; &quot;)) # delete first two rows from the table terry_table &lt;- terry_table[-c(1:2), ] # view output terry_table_col_names The output is quite good, we have combined the first and second row of each column to get a meaningful set of column names. But a few problems remain: there is a space before the S in ‘Sample’ in the first item in the resulting character vector, the fifth item is only a space, with no text, and the sixth item does not have the correct symbols. The leading space in the first item is s nuisance and can easily be removed with the function trimws() which trims white space from the start and end of a character string (but not the internal spaces): terry_table_col_names &lt;- trimws(terry_table_col_names) # see the result terry_table_col_names The empty element in the fifth item is due to the extract_tables() function guessing that there was a fifth column in this table. However, we saw in our earlier inspections that the fifth column contains no values, so it can safely be deleted from the table, and the fifth element of the names can also be deleted: # delete the fifth column terry_table &lt;- terry_table[ , -5] # delete the fifth element of the names vector terry_table_col_names &lt;- terry_table_col_names[-5] The only issue remaining now is the incorrect reading of the characters in the name of the last column. These errors are likely due to subtle differences in the encoding of numbers and letters in the PDF, and the types of encoding that R can easily handle. Encoding is a complex topic relating to the rules a computer follows when it stores human-readable characters as zeros and ones. In any case, the simplest fix is to directly update that item by replacing it with what we can see in the PDF that we got the data from: terry_table_col_names[5] &lt;- &quot;Ring Test Rating (1-6)&quot; If we had a large table with many columns, and most of the column names had an encoding issue like this, we would want an automated method to deal with all the columns at once, rather than directly updating each element by hand as we did here. For example, we could use the parse_character() fucntion from the readr package to convert the encoding to something that looks sensible: terry_table_col_names &lt;- parse_character(terry_table_col_names, locale = locale(encoding = &quot;UTF-8&quot;)) The final step here is to assign the character vector of column names to the column names of the data frame: names(terry_table) &lt;- terry_table_col_names # inspect the result: head(terry_table) That completes the process of fixing the column names for this table, which is a typical set of operations for cleaning a data frame to prepare it for analysis. We still need to convert the column classes for some of the columns from character to numeric, but we will do that in a later section. The second important task relating to fixing names is correcting data entry errors in values in a column. Suppose we have this simple table of artefact by colours, compiled by a group of undergraduate students: # make a simple data frame artefacts_by_colour &lt;- dplyr::data_frame(colour = c(&quot;red&quot;, &quot; green&quot;, &quot;greenish&quot;, &quot;green-like&quot;, &quot;bleu&quot;, &quot;blue&quot;, &quot;Red &quot;), mass_kg = c(3, 5, 8, 4, 2, 1, 7)) # have a look artefacts_by_colour ## # A tibble: 7 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 red 3 ## 2 &quot; green&quot; 5 ## 3 greenish 8 ## 4 green-like 4 ## 5 bleu 2 ## 6 blue 1 ## 7 &quot;Red &quot; 7 At a quick glance at the ‘colour’ column we can see that ‘blue’ is mis-spelled as ‘bleu’, we might want to combine the variants of ‘green’, and ‘red’ appears in the last row with a capital ‘R’, but all the other colour names are in lower case. In a small table like this is a easy to browse the whole column, but in larger tables we can run unique(artefacts_by_colour$colour) to give a character vector of the unique values in the ‘colour’ column, or table(artefacts_by_colour$colour), which returns a table showing each unique value and how many times it occurs in the column. However you discover the problems in a column, the good news is that these are very typical issues that make raw data dirty, and we can easily clean them in R. First, we will fix the case so that all the items in the ‘colour’ column are lower case: artefacts_by_colour$colour &lt;- tolower(artefacts_by_colour$colour) # inspect the result artefacts_by_colour ## # A tibble: 7 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 red 3 ## 2 &quot; green&quot; 5 ## 3 greenish 8 ## 4 green-like 4 ## 5 bleu 2 ## 6 blue 1 ## 7 &quot;red &quot; 7 That has fixed the ‘red’ in the last row, and now we will fix the spelling mistake using the if_else() function. This function is from the dplyr package, there is also an ifelse() in base R, but I prefer the dplyr version because it is more strict, predictable and faster: library(dplyr) artefacts_by_colour$colour &lt;- with(artefacts_by_colour, if_else(colour == &quot;bleu&quot;, &quot;blue&quot;, colour)) # inspect the result artefacts_by_colour ## # A tibble: 7 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 red 3 ## 2 &quot; green&quot; 5 ## 3 greenish 8 ## 4 green-like 4 ## 5 blue 2 ## 6 blue 1 ## 7 &quot;red &quot; 7 The if_else() function is very useful for data cleaning because we can use it to easily update values in a column. It works by evaluating a condition for each item in a vector, one-by-one, and retuning a new vector with values that depend on how each item is evaluated. In this case the condition is colour == &quot;bleu&quot;, which we can translate as ‘is the value of the column ’colour’ equivalent to “bleu”?‘. For each item in the ’colour’ column, the if_else() function will evaluate that condition and return either TRUE (the value is equalivant to “bleu”) or FALSE (the value is not “bleu”, but something else, like “red”, or “green”). The second argument to if_else() is the value to return if the condition is TRUE. In our example, we can translate this as ‘if the value of the column ’colour’ is “bleu”, then return the value “blue”‘. Or more plainly ’where “bleu” occurs in the ’colour’ column, replace it with “blue”‘, akin to a find-and-replace task you might do in a word processing document. The last argument to if_else() is the value to return if the condition is FALSE. In our example, this value is whatever the value of the ’colour’ column is. So when the if_else() gets to the last item in the ‘colour’ column, it sees the value “red”, and it evaluates the condition as ‘is “red” equivalent to “bleu”?’ and returns FALSE, and then skips over the second argument (what to return if TRUE) and looks at the third argument (what to return if FALSE), sees the column name ‘colour’ and returns the last value of that column, which is “red”. Effectively, it leaves the column values unchanged if the condition is FALSE. Note the use of with() in the example above, it saves me from having to type the name of the data frame twice. Without with() I would have typed it like this, repeating the name of the data frame each time I refer to the ‘colour’ column: artefacts_by_colour$colour &lt;- if_else(artefacts_by_colour$colour == &quot;bleu&quot;, &quot;blue&quot;, artefacts_by_colour$colour) We could use a pair of if_else() functions, one nested inside the other, to change ‘greenish’ and ‘green-like’, but instead will will use a simpler option. We can use the function gsub(), which stands for ‘global substituion’, and works by searching through each element in a vector for a match to a pattern that we supply, and then when it finds a match to that pattern, substitutes the element with a replacement that we supply. Below, we supply that pattern to match as &quot;green.*“, which translates as ‘the word green, followed by any character (indicated by the period), any number of times (indicated by the plus symbol)’. In this context, the period and plus symbol are metacharacters that have special meanings instead of their usual literal meanings. The pattern that we have supplied, using this metacharacters is called a ‘regular expression’. Working with regular expressions gives you access to a powerful, flexible, and efficient system for cleaning and manipulating data (see Fitzgerald (2012) and Friedl (2002) for detailed introductions). Regular expressions are used for processing strings in many programming languages, and are notorius for their terse and cryptic appearance. However, learning to work with regular expressions is a worthwhile investment because they can save a lot of time when cleaning data. The code below shows how we use gsub() with a regular expression to replace”greenish&quot; and “green-like” with “green” in the ‘colour’ column: artefacts_by_colour$colour &lt;- gsub(&quot;green.*&quot;, &quot;green&quot;, artefacts_by_colour$colour) # inspect the result artefacts_by_colour ## # A tibble: 7 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 red 3 ## 2 &quot; green&quot; 5 ## 3 green 8 ## 4 green 4 ## 5 blue 2 ## 6 blue 1 ## 7 &quot;red &quot; 7 In the above example we show the use of the period and asterix as metacharacters in a regular expression. If we wanted to be more selective, we could use another regular expression character, the ‘or’ symbol |, to indicate that we only want to subsitute for a specific set of matches: artefacts_by_colour$colour &lt;- gsub(&quot;greenish|green-like&quot;, &quot;green&quot;, artefacts_by_colour$colour) We can group characters used in regular expressions into four categories according to their function: converters, quantifiers, positioners, and operators. Converters are perhaps the most confusing of these, because it is not always obvious what they do and how many of them to use. Table 6.1 demonstrates this challenge where \\\\ seems to have two opposite functions. When working with \\\\ to convert characters in a regular expression, a useful rule of thumb is to keep adding adding backslashes until it works as desired. Table 6.1: Characters commonly used for converting other characters in regular expressions character meaning example \\\\ convert a normal character into a metacharacter &quot;\\\\s&quot; matches any whitespace character (tabs, newlines and spaces); &quot;\\\\S&quot; matches any non-whitespace characters; &quot;\\\\d&quot;?matches any digit; &quot;\\\\D&quot; matches any non-digit character; &quot;\\\\w&quot; matches any word character (uppercase letters, lowercase letters, digits and underscore); &quot;\\\\W&quot; matches any non-word characters (note that we need to double-up the backslash in R, other languages use a single backslash in these contexts) \\\\ convert a metacharacter into a normal (literal) character &quot;\\\\.&quot; matches with a literal period; &quot;\\\\(&quot; matches with a literal left parenthesis Quantifiers specify how many repetitions of the pattern we want to match in a string. Table 6.1 describes the commonly used quantifiers in R’s regular expressions. Table 6.2: Characters commonly used for quantifing characters to match in regular expressions character meaning example * any number of repetition, including zero or more of some character/expression (metacharacter) &quot;*&quot; matches anything combination of characters + 1 or more repetitions; 1 or more of some character/expression (metacharacter) &quot;[0-9]+&quot; matches matches many at least digit 1 numbers such as ‘0’, ‘90’, or ‘021442132’ ? expression is optional; 0/1 of some character/expression (metacharacter) &quot;[Gg]ordon( [Vv]\\\\.)? [Cc]hilde&quot; matches gordon childe’ and ‘Gordon V. Childe’ (two backslashes are needed to match with a literal period) {n} matches exactly n times &quot;ap{2}lique&quot; matches ‘applique’ but not ‘aplique’ or ‘appplique’ {n,} matches at least n times &quot;ap{2,}lique&quot; matches ‘applique’ and ‘appplique’, but not ‘aplique’ {m, n} interval quantifier, allows specifying the minimum and maximum number of matches (metacharacter); {n,m} matches between n and m times. &quot; [1-2]{1}[0-9]{3} &quot; matches years in a sentence, i.e. a space, followed by 1 or 2, followed by three of any of the digits between 0 and 9, followed by a space. It will match ’ 1984 ‘, but not’ 5000 ’ or ’ 2e ’. Positioners, or anchors, indicate the position of a pattern to match within a string. Table 6.3 summarisese some common positioners. These are especially useful for getting a match only at the start or end of a character string. Table 6.3: Characters commonly used match at specific positions in regular expressions character meaning example ^ start of the line (metacharacter) &quot;^text&quot; matches lines such as ‘text’ $ end of the line (metacharacter) &quot;text$&quot; matches lines such as ‘…text’ \\b empty string at either edge of a word. Don’t confuse it with &quot;^ $&quot; which marks the edge of a string. &quot;\\\\bpost&quot; will match ‘post-processual’ and’ post hole’, but not ‘imposter’ \\B empty string provided it is not at an edge of a word &quot;\\\\Bpost&quot; will match ‘imposter’ but not ‘post-processual’ and ‘post hole’ Operators are the core of a regular expression pattern, helping to define what to include in the pattern or exclude from it. Table 6.3 summarises some common positioners Table 6.4: Operators commonly used to make regular expression patterns character meaning example . any character (metacharacter) &quot;8.11&quot; matches 8/11, 0811, 8-11, etc [...] set of characters that will be accepted in the match (character class) &quot;^[Ii]&quot; matches both of these lines: ‘I dug at…’ or i dug at…’ [0-9] searches for a a range of characters (character class) &quot;[a-zA-Z]&quot; will match any letter in upper or lower case, &quot;[0-9]&quot; will match any digit [^...] when used at beginning of character class, “^” means “not” (metacharacter) &quot;[^?.]$&quot; matches any line that does not end in ‘.’ or ‘?’. Will match ‘How old is it!’ but not ‘How old is it?’ | “or”, used to combine subexpressions called alternatives (metacharacter) &quot;^([Nn]eolithic|[Pp]al(ae|e)olithic)&quot; matches any character strings that start with lower/upper case ‘Neolithic’ and ‘Palaeolithic’ and ‘Paleolithic…’ (alternate spelling). (...) define group as the the text in parentheses, groups will be remembered and can be referred to by \\1, \\2, etc. &quot;([A-Z]\\\\.)&quot;, &quot;\\\\1&quot; matches any single capital letter, followed by a single literal period, anywhere in the character string (e.g. a person’s middle initial) An good guide to regular expresions is Fitzgerald (2012), and for R in particular, the chapter on strings in Wickham and Grolemund (2016) is excellent (and is online here: http://r4ds.had.co.nz/strings.html). There are also many useful websites summarising common uses of regular expressions in R, such as http://stat545.com/block022_regular-expression.html, as well as sites to learn, build and test your regular expressions (e.g. http://www.regexr.com/, https://regex101.com/, and https://www.debuggex.com/). These sites are useful for learning more about regular expressions and understanding specific patterns, but it is easy to get confused with subtle differences in the way various programming languages interpret patterns on these websites. I find that there is no substitute for experimenting in my R console with trial and error on a very small example that represents the operation I want to apply to the full data set. Table 6.5 shows the R functions that I most commonly use when working with regular expressions in R. Table 6.5: Functions commonly used to work with regular expressions. The stringr package has many other functions in addition to these that make data clearning easy and fast. Task Functions Identify match to a pattern grep(..., value = FALSE),grepl(),stringr::str_detect() Extract match to a pattern grep(..., value = TRUE),stringr::str_extract(),stringr::str_extract_all() Locate pattern within a string, i.e. give the start position of matched patterns stringr::str_locate(),string::str_locate_all() Replace a pattern gsub(),stringr::str_replace(),stringr::str_replace_all() Split a string using a pattern strsplit(),stringr::str_split() Regular expressions are useful for many other data cleaning tasks besides fixing names, and we will see them pop up in other contexts. However, it can be templing to try to use regular expressions where another approach may be better suited. For example, you might try to write a complex regular expression where a series of simpler regular expressions or even if_else() statements might be easier to write and understand. When you get stuck with regular expressions, take a moment to reflect on simpler options, perhaps breaking the problem into smaller pieces and tackling each piece one at a time. To return to our small table of artefacts, we are nearly done with fixing the problems in with the colour names. The data frame looks good when we view it in the console, but a final check with table(artefacts_by_colour$colour) reveals that we still have some inconsistences in some of the colour names. # check the data frame artefacts_by_colour ## # A tibble: 7 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 red 3 ## 2 &quot; green&quot; 5 ## 3 green 8 ## 4 green 4 ## 5 blue 2 ## 6 blue 1 ## 7 &quot;red &quot; 7 # final check to see if anything else needs fixing table(artefacts_by_colour$colour) ## ## green blue green red red ## 1 2 2 1 1 The output of table() shows that we still have two distinct names for ‘green’ and also for ‘red’, yet we cannot see any obvious differences in these names. To take a closer look, we must print the vector in isolation: artefacts_by_colour$colour ## [1] &quot;red&quot; &quot; green&quot; &quot;green&quot; &quot;green&quot; &quot;blue&quot; &quot;blue&quot; &quot;red &quot; Now we can see the problem - there are leading and trailing spaces that are revealed by the double quotation marks. Those single spaces mean that R sees &quot; green&quot; as distinct from “green”. An easy and fast way to remove these is with the str_trim() function from Hadley Wickham’s stringr package. After that we can use table() to confirm that the colour names are as expected: library(stringr) artefacts_by_colour$colour &lt;- str_trim(artefacts_by_colour$colour) # check that the colour names are as expected table(artefacts_by_colour$colour) ## ## blue green red ## 2 3 2 In situations where you have a large number of columns we can use map_df() to automatically apply functions such as str_trim() to all the columns in a data frame, for example: map_df(artefacts_by_colour, ~str_trim(.x)) ## # A tibble: 7 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;chr&gt; ## 1 red 3 ## 2 green 5 ## 3 green 8 ## 4 green 4 ## 5 blue 2 ## 6 blue 1 ## 7 red 7 However, this has the undesirable side-effect of coercing all columns into the character class, notice in the above output that the ‘mass_kg’ is now a character column. To trim the white space on only the character coloumns, andleave the numeric columns untouched, we can use map_if() and specify a condition that the columns must satisfy to determine if the function will be applied: map_if(artefacts_by_colour, is.character, ~str_trim(.x)) %&gt;% as_data_frame() ## # A tibble: 7 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 red 3 ## 2 green 5 ## 3 green 8 ## 4 green 4 ## 5 blue 2 ## 6 blue 1 ## 7 red 7 The condition we apply here is is.character() which tests to see if each column is character class. If the column is character class, then the str_trim() function is applied to it. The output from map_if() is a list, so we need to add as_data_frame() to convert the list back into a data frame, ready for the next step. 6.3.2 Converting column classes As we saw in the last example, sometimes functions convert our columns to classes that are unexpected or at least not convienent for our analysis. This is a common issue that can cause frustrations with data anlysis, so it is worth taking some time to check your column classes periodically during data clearning and analysis, and being familiar with quick and easy methods to change column classes. For the simplest case, a single column, we can use as.numeric() to coerce a column of numbers from character class to numeric class. Conisider the previous table of artefact colours and masses, and imagine that one more artefact has been added to the data. Notice that the mass value has been mistakenly entered as ‘2.5.’ with an extra period at the end: artefacts_by_colour &lt;- rbind(artefacts_by_colour, c(&quot;yellow&quot;, &quot;2.5.&quot;)) # have a look artefacts_by_colour ## # A tibble: 8 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;chr&gt; ## 1 red 3 ## 2 green 5 ## 3 green 8 ## 4 green 4 ## 5 blue 2 ## 6 blue 1 ## 7 red 7 ## 8 yellow 2.5. When we read this table into R, this ‘mass_kg’ column is now a character column and our attempts to do numerical operations, like calculate the average, will fail. In a simple case like this, where we can see digits but the class is character, as.numeric() is a good choice: artefacts_by_colour$mass_kg &lt;- as.numeric(artefacts_by_colour$mass_kg) # see the result artefacts_by_colour ## # A tibble: 8 x 2 ## colour mass_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 red 3 ## 2 green 5 ## 3 green 8 ## 4 green 4 ## 5 blue 2 ## 6 blue 1 ## 7 red 7 ## 8 yellow NA The key thing to notice here is that while the ‘mass_kg’ column has been converted to a numeric (of the type ‘double’), the last item, which was ‘2.5.’ is now “NA”, which stands for ‘not available’. This is a special or ‘reserved’ word in the R language that represents a missing value. Is is important to be aware of missing values in your data because some common functions (such as mean()) will return only NA if there is even one missing value, unless you specifically instruct the function to remove the NA values (e.g. mean(x, na.rm = TRUE) will remove the NAs so that a mean can be calculated). In some cases data are genuinely missing, for example if the last item in the ‘mass_kg’ column was ‘??’, then all we know is that someone entered some data, but it’s not meaningful data about the mass of the artefact. But since we have ‘2.5.’ we can reasonably infer that the correct value is ‘2.5’. We can use a regular expresion to handle this situation, for example, here is a fairly comprehensive pattern that will extract numbers (including decimal values) from a wide variety of situations in a character vector: library(stringr) x &lt;- c(&quot;2.5.&quot;, &quot;-100.001&quot;, &quot;1.1kg&quot;, &quot;p.10&quot;, &quot;10kg&quot;, &quot;1.0p5&quot;, &quot;1p0.5&quot;) str_extract(x, &quot;-?\\\\.?\\\\d+\\\\.*\\\\d*&quot;) ## [1] &quot;2.5&quot; &quot;-100.001&quot; &quot;1.1&quot; &quot;.10&quot; &quot;10&quot; &quot;1.0&quot; ## [7] &quot;1&quot; It is often helpful to annotate a complex regular expression to show the role of each character: str_extract(x, &quot;-?\\\\.?\\\\d+\\\\.*\\\\d*&quot;) # | | | | | # | | | | \\\\d* matches any digit, zero or more times # | | | \\\\.* matches a literal period, zero or more times # | | \\\\d+ matches any digit one or more times # | \\\\.? matches a literal period, zero or one time, in case # | of the decimal with no leading zero # -? matches a negative sign, zero or one time This pattern captures numbers in several typical mis-entered forms, including when units are accidently added, or stray characters appear at the beginning or end of the number. It does not do well with more ambiguous cases such as the last two items where there is a character in the middle of the number. Indeed, even as humans we cannot easily determine if ‘1.0p5’ is meant to be ‘1.005’ or ‘1.05’. We would need to consider the context of the data recording in detail to make a decision about how to handle values like that (e.g. Was the instrument even capable of reporting a mass to the nearest 0.005 kg? Can we round up to the nearest 0.5 kg wihout any substantial loss of data?). Such a complex regular expression as the previous example may cause more problems that it solves, and we may prefer a simpler, case-by-case approach for handling badly formed values. To solve only the problem of unwanted trailing characters, such a ‘2.5.’ and ‘10kg’, we can use a simpler regular expresion: x &lt;- c(&quot;2.5.&quot;, &quot;-100.001&quot;, &quot;1.1kg&quot;, &quot;p.10&quot;, &quot;10kg&quot;, &quot;1.0p5&quot;, &quot;1p0.5&quot;) str_extract(x, &quot;\\\\d+\\\\.*\\\\d*&quot;) ## [1] &quot;2.5&quot; &quot;100.001&quot; &quot;1.1&quot; &quot;10&quot; &quot;10&quot; &quot;1.0&quot; &quot;1&quot; Here it is with annotations: str_extract(x, &quot;\\\\d+\\\\.*\\\\d*&quot;) # | | | # | | \\\\d* matches any digit, zero or more times # | \\\\.* matches a literal period, zero or more times # \\\\d+ matches any digit one or more times This works well for cases like ‘2.5.’ and ‘10kg’. Although it is not helpful for other types of mistakes, we might prefer to fix those with seperate regular expression functions. To return to our example data frame with ‘2.5.’ in the ‘mass_kg’ column, we can now extract a number from the commonly mis-typed values, and with as.numeric() we can coerce the column to numeric class, ready for analysis: artefacts_by_colour$mass_kg &lt;- as.numeric(str_extract(artefacts_by_colour$mass_kg, &quot;\\\\d+\\\\.*\\\\d*&quot;)) # inspect the output artefacts_by_colour In some situations, such as when you have much larger data frames, it may not be practical to attend to each mis-typed value and extract the numeric value, as we did in the example above. With a larger data set you may be more tolerant of missing values, and main task is simply to get all the columns into the most convienent class, and deal with the NA values later. The type_convert() function from the readr package solves this problem by automatically sampling a bunch of rows in each column and guessing what the class is, and then coercing the column to that class. Consider the ‘terry_table’ data frame that we worked on earlier in the chapter to fix the column names. Last we looked at it all the columns were character class, although columns 2-4 clearly contain numbers. We can use type_convert() to quickly and easily fix the column classes: library(readr) terry_table &lt;- type_convert(terry_table) # inspect the output str(terry_table) An especially nice detail about type_convert() is that is also automatically trims leading and trailing white spaces, solving the problem we saw above with &quot; green&quot; and “green”. Thetype_convert() function is automatically applied when you use any of the the read_*() functions from the readr package (e.g. read_csv() and read_table2()), so that can save some time and frustration by choosing one of those functions to import your data. Finally we have a table that is basically suitable for visualisation and analysis, so we should save it as a CSV ready for the next steps, and so we can deposit it in a repository where it can easilty be reused by others: write_csv(terry_table, &quot;data/data_output/terry_table.csv&quot;) 6.3.3 Splitting and combining columns If we look carefully at the first column of ‘terry_table’, we see sample identifiers that area combination of letters and number, and an dash as a separator. During analysis it is often useful to have each item in this identifier is a separate column so we can do group-wise comparisons. This is a common situation when samples are labelled with a string that combines several pieces of provenance information, such as abbreviations for site, sector, square, exacavation unit, etc. For example, if we want to compare all the artefacts from one square to another square, then it is convienient to have the square identifier in its own column. The exact combination of items in these identifiers varies from project to project, so here we will explore some flexible techniques for splitting one column into several which will be useful in a variety of situations. The simplest method is one we have seen in the previous chapter, separate() from the tidyr package: library(tidyr) # load the library terry_table_split &lt;- terry_table %&gt;% # start with the data frame separate(Sample, # the column to split c(&quot;a&quot;, &quot;b&quot;), # new column names &quot;-&quot;, # character to split at convert = TRUE) # conver to numeric The separate() function takes a data frame as its first argument (here passed invisibly by the pipe), and then the name of the column that we want to split up, followed by the names of the new columns that will be created by the function (here simply ‘a’ and ‘b’, but could be something like ‘site’, ‘square’, ‘unit’, etc.), then the character to split (here it is ‘-’, but we can aslo supply a regular expression), and finally convert = TRUE which ensures that our column of numbers has the numeric class. Among the other arguments to this function, one that I use occasaionlly is remove = which by default removes the input column from the output data frame. [x] 1. Fixing names: correting spelling, replacing spaces, using ifelse, gsub, regex [x] 2. Types: character to numeric, extract numbers from strings 3. Splitting: separating one col into two or more, after the n-th item 4. Reshaping: wide &lt;-&gt; long changing column names regex to clean data values, remove spaces getting data in the most useful object type (numeric, integer, character, factor, logical) ifelse to update/correct data values splitting one column into two or more combining multiple columns into one for unique IDs adding new columns based on calculations of other columns 6.4 Wide and long data Wide data - better for data entry Long data - better for data analysis and viz tidyr gather/spread 6.5 Dealing with missing data removing rows/columns with missing data imputing missing values by mean value, adjacent value, ifelse for arbitrary values filling in with previous values 6.6 Joining data together bind_rows, bind_cols left_join References "],
["visualisatingthedata.html", "Chapter 7 Exploring the data with visualisations 7.1 Overview 7.2 Empirical research into effective data visualisation", " Chapter 7 Exploring the data with visualisations 7.1 Overview To prepare for statistical testing it is essential to visualise your data to see .. 7.2 Empirical research into effective data visualisation There are no definitive principles for best practices in poster design, but there is a substantial body of literature on data visualisation. This has resulted in some widely repeated basic principles, often motivated by ethical and aesthetic concerns. An example of an ethical concern is the principle that only sequential data can be connected with a line chart, because the line implies a continuous change between the points. Measurement of categories, such as artefact types, locations and methods, should not be linked by lines (for example, there is no continuous sequence between a stone artefact and an animal bone). A related principle is that when absolute magnitudes of the data are important, the vertical axis should begin at zero (Robbins, 2005; Strange, 2007). Displaying data along a vertical axis that does not include zero can misrepresent the data range and exaggerate the relative magnitude between values. ONOe influential set of ideas come from the minimimalist mantras of Tufte. One of his most frequently quoted principles is “maximize the data-ink ratio, within reason”. ‘Data-ink’ refers to the ink used to show data, and so the data-ink ratio is the amount of data-ink relative to the total ink used in a visualisation. These principles ensure that the data are not hidden or distorted by poor choices or irrelevant elements on the visualisation, and that the reader can appreciate the data in the visualisation without distraction and confusion. The practical consequences of this advice include avoiding three-dimensional charts where are two-dimensional chart will suffice (e.g. bar charts and pie charts). Similarly, omitting ‘chartjunk’ - grids, colours, and artistic elements on the charts - helps to improve the data-ink ratio. While these principles have intuitive appeal and are widely repeated, they can lead to highly minimimalist charts that are difficult to interpret (Tukey 1990). Inbar, Tractinsky and Meyer (2007) and Kulla-Mader (2007) found that standard charts where overwhelmingly preferred over Tufte-style minimalist charts. Kosslyn (1985) and Carswell (1992), have raised the question of how to decide what is data-ink and what is not, concluding that it is frequently highly subjective. Hullman et al., (2011) have suggested that some chartjunk may benefit readers by promoting engagement with the visualisation. For example, Bateman et al. (2010) and Li and Moacdieh (2014) found that subjects who were shown charts with chartjunk had a significantly higher chance of comprehending the message of the chart as compared to non-embellished charts. Kelly (1989) found no difference in immediate recall of information from high and low data-ink charts in a newspaper format. Similarly, McGurgan (2015) found participants reported similar levels of accuracy and mental effort when answering graph comprehension questions using bar graphs and boxplots with varying data-ink ratios. On the other hand, Gillan and Richman (1994; 1992) found empirical support for the principle of data-ink maximization. They found that the percentage of correct interpretations of chart data was significantly lower for the low data-ink charts compared to medium and high data-ink charts. This brief summary of research into the data-ink ratio shows that it is a problematic concept, with only equivocal empirical support. In sum, it seems that principles based on subjective issues of graph aesthetics, often seen Tufte-style graph designs, do not always lead to the most effective visualisations. These mixed findings suggest that aesthetic minimalism might not always ensure that our data visualisations are easy to interpret accurately. What, then, are the basic principles for optimising the speed at which a reader can percieve the patterns in the data, and the accuracy of the information that a reader can extract from the visualisation? In the context of a poster presentation these optimisations are highly desirable, as the reader of a poster is typically hoping to get information from a poster much quicker than if they were sitting down reading a scholarly article. Cleveland and McGill (1984) conducted experiments using several common types of data visualisations to test the accuracy with which subjects could read point-values and make comparisons in the data. They found that chart types based on length (such as dot charts and bar charts) were read much more accurately than chart types based on angle, area or volume (such as pie charts and three dimensional charts). Furthermore, they found that people perform substantially worse on stacked bar charts than on aligned bar charts, and that comparisons between adjacent bars are more accurate than between widely separated bars. Numerous subsequent studies have generally supported these findings (Heer and Bostock 2010; Kosara and Ziemkiewicz 2010; Talbot, Setlur, and Anand 2014). Heer and Bostock (2010) repicated the core results for comparing sizes across categories, and also found that the addition of gridlines on a plot improved accuracy. Kosara and Ziemkiewicz (2010) tested square pie, or waffle charts (a square divided into 10 x 10 = 100 cells) along with pie, stacked bar and donut charts, and found that respondants were more confident of their reading of square pie charts, and more accurate in reading the chart values, compared to the other types. Zubiaga et al. (2015) tested respondants with five types of chart (bar charts showing the average value of the distribution, bee swarms, boxplots, stacked bar charts, and histograms) to determine their relative effectiveness in visualising distributions of variables. They find that histograms are the most complete in terms of details given, as well as being the chart type that leads respondants to the most accurate understanding of the underlying data. Rangecroft (2003) and Schonlau and Peters (2012) found that respondants can read 2D pie charts more accurately than 3D pie charts. Zachs et al. (1998) found a similar result for 2D bar charts over 3D bar charts. Recent experients have cast some light on the traditional rivalry between pie charts and bar charts (Spence 2005). For comparison judgements between categories, bars are more accurately judged than pies (Feldman-Stewart et al. 2000). However, for estimates of the proportion of the whole, pie charts were as accurate as bar charts (Simkin and Hastie 1987; Spence 1990). For pair-wise comparisons, pie and bar charts also perform similarly (Spence and Lewandowsky 1991; Meyer, Shinar, and Leiser 1997). These studies show that under certain conditions, pie charts may be more effective than bar charts. The best choice of chart type depends on the purpose of the chart (Kosslyn and Chabris 1992), and the evidence does not support making a universal perscriptions about chart types. Although these empirical studies demonstrate a complex relationship between chart type and effectiveness, they can provide a simple, if approximate, rank-order of strategies for visualising data. Dot charts and bar charts are generally at the high-ranking end of the spectrum, along with more exotic styles such as waffle charts and histograms. Lower ranking chart types include stacked bar charts, pie charts. Any kind of 3D chart ranks last. References "],
["artefactdata.html", "Chapter 8 Analysing artefact data 8.1 Overview 8.2 Basic analysis tasks 8.3 Basic plotting tasks 8.4 Stone artefacts 8.5 Faunal remains 8.6 Pottery 8.7 Glass", " Chapter 8 Analysing artefact data 8.1 Overview 8.2 Basic analysis tasks selecting columns selecting rows summarising by group tallying 8.3 Basic plotting tasks bar plot: counts only - plain, stacked distribution plotting: histogram, density, boxplot, beeswarm plot scatter plot: with point shape, colour, size facetting interactive plotting colour schemes on plots statistical layers on plots plots to avoid: 3d, pie, lines over discrete categories 8.4 Stone artefacts reduction indeces geometric morphometry orientation 8.5 Faunal remains Species tables NISP MNI glm richness indeces 8.6 Pottery ?? 8.7 Glass "],
["compositional-pca-ca.html", "Chapter 9 compositional, PCA, CA 9.1 Metal", " Chapter 9 compositional, PCA, CA 9.1 Metal "],
["shapes.html", "Chapter 10 shapes?", " Chapter 10 shapes? "],
["stratigraphic.html", "Chapter 11 Working with stratigraphic data from archaeological excavations 11.1 Overview 11.2 Visualising excavation data 11.3 Stratigraphic analysis 11.4 Particle size analysis", " Chapter 11 Working with stratigraphic data from archaeological excavations 11.1 Overview 11.2 Visualising excavation data stratiplot 11.3 Stratigraphic analysis CONISS 11.4 Particle size analysis "],
["spatialdata.html", "Chapter 12 Spatial data and maps 12.1 Overview 12.2 Making maps 12.3 Doing spatial analysis 12.4 ", " Chapter 12 Spatial data and maps 12.1 Overview library(sf) http://strimas.com/r/tidy-sf/ https://walkerke.github.io/2016/12/spatial-pipelines/ http://rpubs.com/mharris/r_mapping_spatial_data https://pakillo.github.io/GISwithR/#12 12.2 Making maps ggplot ggmap scale and north arrow 12.3 Doing spatial analysis polygon area counting points in an area nearest neighbour 12.4 "]
]
